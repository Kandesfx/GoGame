# üöÄ H∆Ø·ªöNG D·∫™N TRAINING ML TR√äN COLAB/KAGGLE

## üìë M·ª§C L·ª§C

1. [T·ªïng quan](#1-t·ªïng-quan)
2. [Setup Colab/Kaggle](#2-setup-colabkaggle)
3. [Thu th·∫≠p d·ªØ li·ªáu chuy√™n nghi·ªáp](#3-thu-th·∫≠p-d·ªØ-li·ªáu-chuy√™n-nghi·ªáp)
4. [X·ª≠ l√Ω d·ªØ li·ªáu](#4-x·ª≠-l√Ω-d·ªØ-li·ªáu)
5. [Quy tr√¨nh Training](#5-quy-tr√¨nh-training)
6. [Deployment Model](#6-deployment-model)

---

## 1. T·ªîNG QUAN

### 1.1. T·∫°i sao d√πng Colab/Kaggle?

| Platform | GPU | Storage | Th·ªùi gian | Gi·ªõi h·∫°n |
|----------|-----|---------|-----------|----------|
| **Google Colab** | ‚úÖ Free T4 (16GB) | 15GB | 12h/session | C·∫ßn reconnect |
| **Kaggle** | ‚úÖ Free P100 (16GB) | 30GB | 9h/session | Stable h∆°n |
| **Local** | ‚ùå C·∫ßn GPU ri√™ng | Unlimited | Unlimited | T·ªën ti·ªÅn |

**Khuy·∫øn ngh·ªã**: D√πng **Kaggle** v√¨ ·ªïn ƒë·ªãnh h∆°n, ho·∫∑c **Colab Pro** ($10/th√°ng) n·∫øu c·∫ßn th·ªùi gian d√†i h∆°n.

### 1.2. Workflow t·ªïng quan

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 1: Download Professional Games           ‚îÇ
‚îÇ  ‚Ä¢ KGS Archive (70K games)                      ‚îÇ
‚îÇ  ‚Ä¢ OGS API (recent games)                       ‚îÇ
‚îÇ  ‚Ä¢ GoGoD (optional, paid)                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 2: Parse SGF ‚Üí Positions                  ‚îÇ
‚îÇ  ‚Ä¢ Extract board states                         ‚îÇ
‚îÇ  ‚Ä¢ Filter quality games                         ‚îÇ
‚îÇ  ‚Ä¢ Generate features (17 planes)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 3: Generate Labels                        ‚îÇ
‚îÇ  ‚Ä¢ Threat maps (rule-based)                      ‚îÇ
‚îÇ  ‚Ä¢ Attack maps (rule-based)                      ‚îÇ
‚îÇ  ‚Ä¢ Intent labels (pattern-based)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 4: Upload to Colab/Kaggle                 ‚îÇ
‚îÇ  ‚Ä¢ Compress dataset                              ‚îÇ
‚îÇ  ‚Ä¢ Upload to Google Drive / Kaggle Dataset       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 5: Train Model                            ‚îÇ
‚îÇ  ‚Ä¢ Load dataset                                  ‚îÇ
‚îÇ  ‚Ä¢ Train multi-task model                       ‚îÇ
‚îÇ  ‚Ä¢ Monitor v·ªõi TensorBoard                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 6: Download Model                         ‚îÇ
‚îÇ  ‚Ä¢ Save checkpoint                               ‚îÇ
‚îÇ  ‚Ä¢ Download v·ªÅ local                            ‚îÇ
‚îÇ  ‚Ä¢ Deploy v√†o backend                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.3. Y√™u c·∫ßu d·ªØ li·ªáu

| Board Size | Min Games | Min Positions | Recommended |
|------------|-----------|---------------|-------------|
| 9√ó9 | 1,000 | 80,000 | 5,000 games |
| 13√ó13 | 500 | 60,000 | 2,000 games |
| 19√ó19 | 2,000 | 400,000 | 10,000 games |

**T·ªïng c·∫ßn**: ~17,000 games chuy√™n nghi·ªáp (t·ª´ rank 5d tr·ªü l√™n)

---

## 2. SETUP COLAB/KAGGLE

### 2.1. Google Colab Setup

#### B∆∞·ªõc 1: T·∫°o Notebook m·ªõi

1. V√†o https://colab.research.google.com
2. File ‚Üí New Notebook
3. ƒê·∫∑t t√™n: `GoGame_ML_Training.ipynb`

#### B∆∞·ªõc 2: Enable GPU

```python
# Cell 1: Check GPU
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
```

**Runtime ‚Üí Change runtime type ‚Üí GPU (T4)**

#### B∆∞·ªõc 3: Mount Google Drive

```python
# Cell 2: Mount Drive
from google.colab import drive
drive.mount('/content/drive')

# T·∫°o th∆∞ m·ª•c l√†m vi·ªác
import os
WORK_DIR = '/content/drive/MyDrive/GoGame_ML'
os.makedirs(WORK_DIR, exist_ok=True)
os.chdir(WORK_DIR)
print(f"Working directory: {WORK_DIR}")
```

#### B∆∞·ªõc 4: Clone Repository (ho·∫∑c upload code)

**Option A: Clone t·ª´ GitHub** (n·∫øu c√≥ repo)
```python
# Cell 3: Clone repo
!git clone https://github.com/yourusername/GoGame.git
%cd GoGame
```

**Option B: Upload code th·ªß c√¥ng**
```python
# Cell 3: Upload files
from google.colab import files
# Upload: src/ml/models/*.py, src/ml/training/*.py, src/ml/features.py
```

#### B∆∞·ªõc 5: Install Dependencies

```python
# Cell 4: Install packages
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install numpy pandas tqdm tensorboard
!pip install sgf  # For parsing SGF files
!pip install go  # If needed for Go utilities
```

### 2.2. Kaggle Setup

#### B∆∞·ªõc 1: T·∫°o Notebook m·ªõi

1. V√†o https://www.kaggle.com/code
2. New Notebook
3. ƒê·∫∑t t√™n: `gogame-ml-training`

#### B∆∞·ªõc 2: Enable GPU

**Settings ‚Üí Accelerator ‚Üí GPU (P100)**

#### B∆∞·ªõc 3: Upload Dataset

1. **Data ‚Üí Add data ‚Üí New dataset**
2. Upload dataset files (s·∫Ω h∆∞·ªõng d·∫´n ·ªü ph·∫ßn sau)
3. Dataset s·∫Ω c√≥ path: `/kaggle/input/your-dataset-name/`

#### B∆∞·ªõc 4: Install Dependencies

```python
# Cell 1: Install packages
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install numpy pandas tqdm tensorboard sgf
```

#### B∆∞·ªõc 5: Setup Working Directory

```python
# Cell 2: Setup
import os
WORK_DIR = '/kaggle/working'
os.chdir(WORK_DIR)

# Copy code files (ho·∫∑c clone repo)
# !git clone https://github.com/yourusername/GoGame.git
```

---

## 3. THU TH·∫¨P D·ªÆ LI·ªÜU CHUY√äN NGHI·ªÜP

### 3.1. Ngu·ªìn d·ªØ li·ªáu

#### 3.1.1. KGS Game Archive (‚≠ê RECOMMENDED - FREE)

**URL**: https://u-go.net/gamerecords/

**Th√¥ng tin**:
- ~70,000 games chuy√™n nghi·ªáp
- Format: SGF
- Ranks: 1d - 9d professional
- **FREE v√† kh√¥ng gi·ªõi h·∫°n**

**Script download** (ch·∫°y tr√™n local tr∆∞·ªõc khi upload l√™n Colab):

```python
# scripts/download_kgs_games.py

import requests
import os
from pathlib import Path
from tqdm import tqdm
import time

KGS_BASE_URL = "https://u-go.net/gamerecords/"
OUTPUT_DIR = Path("data/raw/kgs")

def download_kgs_games(min_rank=5, max_games=10000, output_dir=OUTPUT_DIR):
    """
    Download games t·ª´ KGS Archive
    
    Args:
        min_rank: Minimum rank (dan) - ch·ªâ l·∫•y t·ª´ 5d tr·ªü l√™n
        max_games: S·ªë l∆∞·ª£ng games t·ªëi ƒëa
        output_dir: Th∆∞ m·ª•c l∆∞u
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    
    downloaded = 0
    failed = 0
    
    # KGS c√≥ nhi·ªÅu nƒÉm, m·ªói nƒÉm c√≥ nhi·ªÅu th√°ng
    years = range(2000, 2024)  # 2000-2023
    
    print(f"B·∫Øt ƒë·∫ßu download t·ª´ KGS Archive...")
    print(f"Target: {max_games} games, min rank: {min_rank}d")
    
    for year in years:
        if downloaded >= max_games:
            break
            
        for month in range(1, 13):
            if downloaded >= max_games:
                break
            
            # URL format: https://u-go.net/gamerecords/YYYY/MM/
            url = f"{KGS_BASE_URL}{year}/{month:02d}/"
            
            try:
                # Get list of SGF files
                response = requests.get(url, timeout=10)
                if response.status_code != 200:
                    continue
                
                # Parse HTML ƒë·ªÉ t√¨m links .sgf
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(response.text, 'html.parser')
                
                sgf_links = []
                for link in soup.find_all('a'):
                    href = link.get('href', '')
                    if href.endswith('.sgf'):
                        sgf_links.append(href)
                
                # Download t·ª´ng file
                for sgf_file in sgf_links:
                    if downloaded >= max_games:
                        break
                    
                    sgf_url = f"{url}{sgf_file}"
                    output_path = output_dir / f"{year}_{month:02d}_{sgf_file}"
                    
                    # Skip n·∫øu ƒë√£ c√≥
                    if output_path.exists():
                        continue
                    
                    try:
                        sgf_response = requests.get(sgf_url, timeout=10)
                        if sgf_response.status_code == 200:
                            # Check rank trong SGF metadata
                            sgf_content = sgf_response.text
                            if f"{min_rank}d" in sgf_content or f"{min_rank+1}d" in sgf_content:
                                output_path.write_text(sgf_content, encoding='utf-8')
                                downloaded += 1
                                
                                if downloaded % 100 == 0:
                                    print(f"Downloaded: {downloaded}/{max_games}")
                                
                                time.sleep(0.1)  # Rate limiting
                    except Exception as e:
                        failed += 1
                        if failed % 100 == 0:
                            print(f"Failed downloads: {failed}")
                        continue
                
            except Exception as e:
                print(f"Error processing {year}/{month}: {e}")
                continue
    
    print(f"\n‚úÖ Ho√†n th√†nh!")
    print(f"Downloaded: {downloaded} games")
    print(f"Failed: {failed}")
    print(f"Saved to: {output_dir}")

if __name__ == '__main__':
    download_kgs_games(min_rank=5, max_games=10000)
```

**C√°ch ch·∫°y**:
```bash
# Tr√™n local machine
python scripts/download_kgs_games.py --min-rank 5 --max-games 10000
```

#### 3.1.2. OGS API (FREE)

**URL**: https://online-go.com/api/v1/

**Script download**:

```python
# scripts/download_ogs_games.py

import requests
import json
from pathlib import Path
from tqdm import tqdm

OGS_API_BASE = "https://online-go.com/api/v1/"

def download_ogs_games(min_rank=5, max_games=5000, output_dir=Path("data/raw/ogs")):
    """
    Download games t·ª´ OGS API
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    
    downloaded = 0
    page = 1
    
    print(f"Downloading from OGS API...")
    
    while downloaded < max_games:
        # Get games list
        url = f"{OGS_API_BASE}games/"
        params = {
            'ordering': '-ended',
            'page': page,
            'page_size': 100,
            'ranked': True
        }
        
        try:
            response = requests.get(url, params=params, timeout=10)
            if response.status_code != 200:
                break
            
            data = response.json()
            games = data.get('results', [])
            
            if not games:
                break
            
            for game in games:
                if downloaded >= max_games:
                    break
                
                # Check rank
                black_rank = game.get('black', {}).get('ranking', 0)
                white_rank = game.get('white', {}).get('ranking', 0)
                
                if black_rank < min_rank * 100 or white_rank < min_rank * 100:
                    continue  # OGS uses numeric ranks (500 = 5d)
                
                # Download SGF
                game_id = game['id']
                sgf_url = f"{OGS_API_BASE}games/{game_id}/sgf"
                
                try:
                    sgf_response = requests.get(sgf_url, timeout=10)
                    if sgf_response.status_code == 200:
                        output_path = output_dir / f"ogs_{game_id}.sgf"
                        output_path.write_text(sgf_response.text, encoding='utf-8')
                        downloaded += 1
                        
                        if downloaded % 100 == 0:
                            print(f"Downloaded: {downloaded}/{max_games}")
                except:
                    continue
            
            page += 1
            
        except Exception as e:
            print(f"Error: {e}")
            break
    
    print(f"‚úÖ Downloaded {downloaded} games from OGS")

if __name__ == '__main__':
    download_ogs_games(min_rank=5, max_games=5000)
```

#### 3.1.3. GoGoD Database (PAID - Optional)

**URL**: https://www.gogodonline.co.uk/

**Th√¥ng tin**:
- ~100,000 historical games
- Very high quality
- Cost: ~$40 one-time
- Format: SGF

**N·∫øu mua**: Download v√† extract v√†o `data/raw/gogod/`

---

## 4. X·ª¨ L√ù D·ªÆ LI·ªÜU

### 4.1. Parse SGF ‚Üí Positions

**File**: `scripts/parse_sgf_to_positions.py`

```python
# scripts/parse_sgf_to_positions.py

import sgf
from pathlib import Path
import torch
from tqdm import tqdm
import numpy as np

def parse_sgf_file(sgf_path):
    """
    Parse 1 SGF file v√† extract t·∫•t c·∫£ positions
    
    Returns:
        List of (board_state, move, outcome) tuples
    """
    try:
        with open(sgf_path, 'r', encoding='utf-8') as f:
            sgf_content = f.read()
        
        # Parse SGF
        game = sgf.parse(sgf_content)
        
        # Extract metadata
        root = game.root
        board_size = int(root.properties.get('SZ', ['19'])[0])
        result = root.properties.get('RE', [''])[0]  # "B+12.5" or "W+R"
        
        # Determine winner
        if result.startswith('B'):
            winner = 'B'
        elif result.startswith('W'):
            winner = 'W'
        else:
            winner = None  # Unknown
        
        # Extract moves
        positions = []
        board = create_empty_board(board_size)
        current_player = 'B'  # Black starts
        
        for node in game.rest:
            # Get move
            if 'B' in node.properties:
                move = node.properties['B'][0]
                color = 'B'
            elif 'W' in node.properties:
                move = node.properties['W'][0]
                color = 'W'
            else:
                continue  # Pass or other
            
            # Parse move coordinate
            if move and move != '' and move != 'tt':  # 'tt' = pass
                x, y = parse_sgf_coord(move, board_size)
                
                # Save position BEFORE move
                positions.append({
                    'board_state': board.copy(),
                    'move': (x, y),
                    'current_player': current_player,
                    'move_number': len(positions),
                    'board_size': board_size,
                    'game_result': result,
                    'winner': winner
                })
                
                # Apply move
                board[y, x] = 1 if color == 'B' else 2
                # TODO: Apply Go rules (captures, ko, etc.)
            
            current_player = 'W' if current_player == 'B' else 'B'
        
        return positions
        
    except Exception as e:
        print(f"Error parsing {sgf_path}: {e}")
        return []

def parse_sgf_coord(sgf_coord, board_size):
    """
    Convert SGF coordinate to (x, y)
    SGF: 'aa' = (0, 0), 'sa' = (18, 0) for 19x19
    """
    if len(sgf_coord) < 2:
        return None, None
    
    x = ord(sgf_coord[0]) - ord('a')
    y = ord(sgf_coord[1]) - ord('a')
    
    # Skip 'i' (no I in Go coordinates)
    if x >= 8:
        x -= 1
    if y >= 8:
        y -= 1
    
    return x, y

def create_empty_board(size):
    """Create empty Go board"""
    return np.zeros((size, size), dtype=np.int8)

def process_all_sgf_files(sgf_dir, output_path, board_sizes=[9, 13, 19]):
    """
    Process t·∫•t c·∫£ SGF files v√† t·∫°o dataset
    
    Args:
        sgf_dir: Th∆∞ m·ª•c ch·ª©a SGF files
        output_path: Path ƒë·ªÉ l∆∞u PyTorch dataset
        board_sizes: C√°c board sizes c·∫ßn x·ª≠ l√Ω
    """
    sgf_dir = Path(sgf_dir)
    all_positions = {size: [] for size in board_sizes}
    
    sgf_files = list(sgf_dir.glob("*.sgf"))
    print(f"Found {len(sgf_files)} SGF files")
    
    for sgf_file in tqdm(sgf_files, desc="Parsing SGF"):
        positions = parse_sgf_file(sgf_file)
        
        for pos in positions:
            board_size = pos['board_size']
            if board_size in board_sizes:
                all_positions[board_size].append(pos)
    
    # Save datasets
    for board_size in board_sizes:
        if all_positions[board_size]:
            output_file = output_path / f"positions_{board_size}x{board_size}.pt"
            torch.save({
                'positions': all_positions[board_size],
                'board_size': board_size,
                'total': len(all_positions[board_size])
            }, output_file)
            print(f"‚úÖ Saved {len(all_positions[board_size])} positions for {board_size}x{board_size}")
    
    return all_positions

if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--input', type=str, required=True, help='SGF directory')
    parser.add_argument('--output', type=str, default='data/processed', help='Output directory')
    parser.add_argument('--board-sizes', type=int, nargs='+', default=[9, 13, 19])
    
    args = parser.parse_args()
    
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    process_all_sgf_files(args.input, output_dir, args.board_sizes)
```

**C√°ch ch·∫°y**:
```bash
python scripts/parse_sgf_to_positions.py \
  --input data/raw/kgs \
  --output data/processed \
  --board-sizes 9 13 19
```

### 4.2. Generate Features (17 Planes)

**File**: `src/ml/features.py` (ƒë√£ c√≥, c·∫ßn update)

```python
# src/ml/features.py (update)

import torch
import numpy as np
from typing import Tuple

def board_to_tensor(board_state: np.ndarray, current_player: str, board_size: int) -> torch.Tensor:
    """
    Convert board state to 17-plane tensor
    
    Args:
        board_state: (board_size, board_size) array, 0=empty, 1=black, 2=white
        current_player: 'B' or 'W'
        board_size: Board size
    
    Returns:
        Tensor of shape (17, board_size, board_size)
    """
    features = torch.zeros((17, board_size, board_size), dtype=torch.float32)
    
    # Plane 0: Black stones
    features[0] = (board_state == 1).float()
    
    # Plane 1: White stones
    features[1] = (board_state == 2).float()
    
    # Plane 2-7: Liberty counts (simplified - c·∫ßn implement ƒë·∫ßy ƒë·ªß)
    # TODO: Calculate actual liberties for each stone
    # For now, use simple heuristics
    
    # Plane 8-15: History (last 4 moves, 2 planes each)
    # TODO: Track move history
    
    # Plane 16: Turn indicator
    features[16] = 1.0 if current_player == 'B' else 0.0
    
    return features

def process_positions_to_features(positions_data, board_size):
    """
    Convert positions to feature tensors
    
    Args:
        positions_data: List of position dicts from parse_sgf
        board_size: Board size
    
    Returns:
        List of feature tensors
    """
    features_list = []
    
    for pos in positions_data:
        board_state = pos['board_state']
        current_player = pos['current_player']
        
        features = board_to_tensor(board_state, current_player, board_size)
        features_list.append(features)
    
    return features_list
```

### 4.3. Generate Labels

**File**: `scripts/generate_labels.py`

```python
# scripts/generate_labels.py

import torch
import numpy as np
from pathlib import Path
from tqdm import tqdm

def generate_threat_map(board_state, current_player):
    """
    Generate threat map using rule-based heuristics
    
    Returns:
        (board_size, board_size) tensor, values 0-1
    """
    board_size = board_state.shape[0]
    threat_map = np.zeros((board_size, board_size), dtype=np.float32)
    
    # TODO: Implement threat detection
    # - Groups with 1 liberty ‚Üí 1.0
    # - Groups with 2 liberties ‚Üí 0.7
    # - False eyes ‚Üí 0.6
    # - Cutting points ‚Üí 0.5
    
    return torch.from_numpy(threat_map)

def generate_attack_map(board_state, current_player):
    """
    Generate attack opportunity map
    
    Returns:
        (board_size, board_size) tensor, values 0-1
    """
    board_size = board_state.shape[0]
    attack_map = np.zeros((board_size, board_size), dtype=np.float32)
    
    # TODO: Implement attack detection
    # - Opponent in atari ‚Üí 1.0
    # - Can cut ‚Üí 0.8
    # - Invasion points ‚Üí 0.6
    # - Ladder works ‚Üí 0.7
    
    return torch.from_numpy(attack_map)

def generate_intent_label(board_state, move, prev_moves):
    """
    Generate intent label from move pattern
    
    Returns:
        intent_type: str ('territory', 'attack', 'defense', 'connection', 'cut')
        confidence: float
    """
    # TODO: Implement intent recognition
    # - Pattern matching
    # - Heuristic analysis
    
    return 'attack', 0.5  # Placeholder

def process_dataset_with_labels(input_path, output_path):
    """
    Process dataset v√† generate labels
    """
    print(f"Loading positions from {input_path}...")
    data = torch.load(input_path)
    positions = data['positions']
    board_size = data['board_size']
    
    print(f"Processing {len(positions)} positions...")
    
    labeled_data = []
    
    for pos in tqdm(positions, desc="Generating labels"):
        board_state = pos['board_state']
        current_player = pos['current_player']
        move = pos['move']
        
        # Generate features
        features = board_to_tensor(board_state, current_player, board_size)
        
        # Generate labels
        threat_map = generate_threat_map(board_state, current_player)
        attack_map = generate_attack_map(board_state, current_player)
        intent_type, intent_conf = generate_intent_label(board_state, move, [])
        
        labeled_data.append({
            'features': features,
            'threat_map': threat_map,
            'attack_map': attack_map,
            'intent': {
                'type': intent_type,
                'confidence': intent_conf
            },
            'metadata': {
                'move_number': pos['move_number'],
                'game_result': pos['game_result'],
                'winner': pos['winner']
            }
        })
    
    # Save
    torch.save({
        'labeled_data': labeled_data,
        'board_size': board_size,
        'total': len(labeled_data)
    }, output_path)
    
    print(f"‚úÖ Saved labeled dataset to {output_path}")
    print(f"Total samples: {len(labeled_data)}")

if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--input', type=str, required=True)
    parser.add_argument('--output', type=str, required=True)
    
    args = parser.parse_args()
    process_dataset_with_labels(args.input, args.output)
```

### 4.4. Prepare Dataset cho Colab/Kaggle

**Sau khi x·ª≠ l√Ω xong, compress v√† upload**:

```bash
# Compress dataset
cd data/processed
tar -czf gogame_dataset.tar.gz *.pt
# Ho·∫∑c zip
zip -r gogame_dataset.zip *.pt

# Upload l√™n Google Drive (cho Colab)
# Ho·∫∑c upload l√™n Kaggle Dataset (cho Kaggle)
```

---

## 5. QUY TR√åNH TRAINING

### 5.1. Setup tr√™n Colab

**Notebook structure**:

```python
# ============================================
# CELL 1: Setup & Install
# ============================================

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from pathlib import Path
from tqdm import tqdm
import os

# Check GPU
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}")

# Mount Drive
from google.colab import drive
drive.mount('/content/drive')

# Setup paths
WORK_DIR = Path('/content/drive/MyDrive/GoGame_ML')
WORK_DIR.mkdir(exist_ok=True)
os.chdir(WORK_DIR)

# Install packages
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install numpy pandas tqdm tensorboard sgf

print("‚úÖ Setup complete!")
```

```python
# ============================================
# CELL 2: Load Dataset
# ============================================

# Upload dataset file ho·∫∑c load t·ª´ Drive
dataset_path = WORK_DIR / 'gogame_dataset_9x9.pt'

# Load dataset
print("Loading dataset...")
dataset = torch.load(dataset_path)
labeled_data = dataset['labeled_data']
board_size = dataset['board_size']

print(f"‚úÖ Loaded {len(labeled_data)} samples")
print(f"Board size: {board_size}x{board_size}")

# Split train/val/test
from sklearn.model_selection import train_test_split

train_data, temp_data = train_test_split(
    labeled_data, 
    test_size=0.2, 
    random_state=42
)
val_data, test_data = train_test_split(
    temp_data,
    test_size=0.5,
    random_state=42
)

print(f"Train: {len(train_data)}")
print(f"Val: {len(val_data)}")
print(f"Test: {len(test_data)}")
```

```python
# ============================================
# CELL 3: Create Dataset Class
# ============================================

class GoPositionDataset(Dataset):
    def __init__(self, data, augment=False):
        self.data = data
        self.augment = augment
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        
        features = sample['features']
        threat_map = sample['threat_map']
        attack_map = sample['attack_map']
        intent = sample['intent']
        
        # TODO: Add augmentation if self.augment
        
        return {
            'features': features,
            'threat_map': threat_map,
            'attack_map': attack_map,
            'intent_type': intent['type'],
            'intent_conf': intent['confidence']
        }

# Create datasets
train_dataset = GoPositionDataset(train_data, augment=True)
val_dataset = GoPositionDataset(val_data, augment=False)
test_dataset = GoPositionDataset(test_data, augment=False)

# Create dataloaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)

print("‚úÖ Datasets created")
```

```python
# ============================================
# CELL 4: Load Model Architecture
# ============================================

# Import model code (upload files tr∆∞·ªõc)
import sys
sys.path.append(str(WORK_DIR))

# Copy model files v√†o Colab
# Ho·∫∑c import t·ª´ uploaded files
from src.ml.models.multi_task_model import MultiTaskModel
from src.ml.models.shared_backbone import SharedBackbone
from src.ml.models.threat_head import ThreatHead
from src.ml.models.attack_head import AttackHead
from src.ml.models.intent_head import IntentHead

# Create model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = MultiTaskModel(board_size=9).to(device)

print(f"‚úÖ Model created on {device}")
print(f"Parameters: {sum(p.numel() for p in model.parameters()):,}")
```

```python
# ============================================
# CELL 5: Training Loop
# ============================================

# Config
config = {
    'num_epochs': 50,
    'learning_rate': 1e-3,
    'weight_decay': 1e-4,
    'patience': 10,
    'checkpoint_dir': WORK_DIR / 'checkpoints',
    'log_dir': WORK_DIR / 'logs'
}

config['checkpoint_dir'].mkdir(exist_ok=True)
config['log_dir'].mkdir(exist_ok=True)

# Optimizer
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=config['learning_rate'],
    weight_decay=config['weight_decay']
)

# Scheduler
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max=config['num_epochs']
)

# Loss functions
threat_loss_fn = nn.MSELoss()
attack_loss_fn = nn.MSELoss()
intent_loss_fn = nn.CrossEntropyLoss()

# TensorBoard
writer = SummaryWriter(config['log_dir'])

# Training
best_val_loss = float('inf')
patience_counter = 0

for epoch in range(config['num_epochs']):
    print(f"\n=== Epoch {epoch+1}/{config['num_epochs']} ===")
    
    # Train
    model.train()
    train_loss = 0
    train_threat_loss = 0
    train_attack_loss = 0
    train_intent_loss = 0
    
    for batch in tqdm(train_loader, desc='Training'):
        features = batch['features'].to(device)
        threat_map = batch['threat_map'].to(device)
        attack_map = batch['attack_map'].to(device)
        intent_type = batch['intent_type']  # TODO: Convert to class index
        
        optimizer.zero_grad()
        
        # Forward
        outputs = model(features)
        
        # Losses
        threat_loss = threat_loss_fn(outputs['threat_map'], threat_map)
        attack_loss = attack_loss_fn(outputs['attack_map'], attack_map)
        # intent_loss = intent_loss_fn(outputs['intent_logits'], intent_type)
        
        total_loss = threat_loss + attack_loss  # + intent_loss
        
        # Backward
        total_loss.backward()
        optimizer.step()
        
        # Track
        train_loss += total_loss.item()
        train_threat_loss += threat_loss.item()
        train_attack_loss += attack_loss.item()
    
    # Average
    train_loss /= len(train_loader)
    train_threat_loss /= len(train_loader)
    train_attack_loss /= len(train_loader)
    
    # Validate
    model.eval()
    val_loss = 0
    val_threat_loss = 0
    val_attack_loss = 0
    
    with torch.no_grad():
        for batch in tqdm(val_loader, desc='Validating'):
            features = batch['features'].to(device)
            threat_map = batch['threat_map'].to(device)
            attack_map = batch['attack_map'].to(device)
            
            outputs = model(features)
            
            threat_loss = threat_loss_fn(outputs['threat_map'], threat_map)
            attack_loss = attack_loss_fn(outputs['attack_map'], attack_map)
            total_loss = threat_loss + attack_loss
            
            val_loss += total_loss.item()
            val_threat_loss += threat_loss.item()
            val_attack_loss += attack_loss.item()
    
    val_loss /= len(val_loader)
    val_threat_loss /= len(val_loader)
    val_attack_loss /= len(val_loader)
    
    # Log
    writer.add_scalar('Loss/Train', train_loss, epoch)
    writer.add_scalar('Loss/Val', val_loss, epoch)
    writer.add_scalar('Loss/Train_Threat', train_threat_loss, epoch)
    writer.add_scalar('Loss/Val_Threat', val_threat_loss, epoch)
    writer.add_scalar('Loss/Train_Attack', train_attack_loss, epoch)
    writer.add_scalar('Loss/Val_Attack', val_attack_loss, epoch)
    writer.add_scalar('LR', scheduler.get_last_lr()[0], epoch)
    
    print(f"Train Loss: {train_loss:.4f} (Threat: {train_threat_loss:.4f}, Attack: {train_attack_loss:.4f})")
    print(f"Val Loss: {val_loss:.4f} (Threat: {val_threat_loss:.4f}, Attack: {val_attack_loss:.4f})")
    
    # Save best model
    if val_loss < best_val_loss - 1e-4:
        best_val_loss = val_loss
        patience_counter = 0
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_loss': val_loss,
            'config': config
        }
        torch.save(checkpoint, config['checkpoint_dir'] / f'best_model_epoch_{epoch}.pt')
        print(f"‚úÖ Saved best model (val_loss: {val_loss:.4f})")
    else:
        patience_counter += 1
    
    # Early stopping
    if patience_counter >= config['patience']:
        print(f"Early stopping at epoch {epoch}")
        break
    
    # Scheduler step
    scheduler.step()
    
    # Periodic checkpoint
    if (epoch + 1) % 10 == 0:
        torch.save(checkpoint, config['checkpoint_dir'] / f'checkpoint_epoch_{epoch}.pt')

writer.close()
print("\n‚úÖ Training complete!")
```

```python
# ============================================
# CELL 6: Evaluate Model
# ============================================

# Load best model
best_checkpoint = torch.load(config['checkpoint_dir'] / 'best_model_epoch_X.pt')
model.load_state_dict(best_checkpoint['model_state_dict'])

# Evaluate on test set
model.eval()
test_loss = 0

with torch.no_grad():
    for batch in tqdm(test_loader, desc='Testing'):
        features = batch['features'].to(device)
        threat_map = batch['threat_map'].to(device)
        attack_map = batch['attack_map'].to(device)
        
        outputs = model(features)
        
        threat_loss = threat_loss_fn(outputs['threat_map'], threat_map)
        attack_loss = attack_loss_fn(outputs['attack_map'], attack_map)
        total_loss = threat_loss + attack_loss
        
        test_loss += total_loss.item()

test_loss /= len(test_loader)
print(f"Test Loss: {test_loss:.4f}")
```

```python
# ============================================
# CELL 7: Download Model
# ============================================

# Model s·∫Ω t·ª± ƒë·ªông l∆∞u v√†o Google Drive
# Ho·∫∑c download v·ªÅ local:
from google.colab import files

# Download checkpoint
files.download(str(config['checkpoint_dir'] / 'best_model_epoch_X.pt'))
```

---

## 6. DEPLOYMENT MODEL

### 6.1. Download Model v·ªÅ Local

```bash
# T·ª´ Google Drive
# Ho·∫∑c t·ª´ Colab: files.download()

# Save v√†o project
mkdir -p models/ml
cp best_model_epoch_X.pt models/ml/multi_task_9x9.pt
```

### 6.2. Load Model trong Backend

```python
# backend/app/services/ml_analysis_service.py

import torch
from pathlib import Path
from src.ml.models.multi_task_model import MultiTaskModel

class MLAnalysisService:
    def __init__(self, model_path: Path):
        self.device = torch.device('cpu')  # Ho·∫∑c 'cuda' n·∫øu c√≥ GPU
        self.model = MultiTaskModel(board_size=9)
        
        # Load checkpoint
        checkpoint = torch.load(model_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()
        
        print(f"‚úÖ Loaded ML model from {model_path}")
    
    def analyze(self, board_state):
        """Run inference"""
        with torch.no_grad():
            outputs = self.model(board_state)
        return outputs
```

---

## 7. CHECKLIST HO√ÄN CH·ªàNH

### Phase 1: Data Collection (Local)
- [ ] Download KGS games (10,000+ games)
- [ ] Download OGS games (5,000+ games)
- [ ] Parse SGF ‚Üí Positions
- [ ] Filter quality games (5d+)
- [ ] Generate features (17 planes)
- [ ] Generate labels (threat, attack, intent)
- [ ] Split train/val/test
- [ ] Compress dataset

### Phase 2: Upload to Colab/Kaggle
- [ ] Upload dataset to Google Drive / Kaggle Dataset
- [ ] Upload model code files
- [ ] Setup Colab/Kaggle notebook

### Phase 3: Training
- [ ] Load dataset
- [ ] Create DataLoader
- [ ] Initialize model
- [ ] Train model
- [ ] Monitor v·ªõi TensorBoard
- [ ] Save checkpoints
- [ ] Evaluate on test set

### Phase 4: Deployment
- [ ] Download model
- [ ] Test model locally
- [ ] Integrate v√†o backend
- [ ] Deploy to production

---

**END OF PART 1**

*Ti·∫øp t·ª•c v·ªõi ph·∫ßn chi ti·∫øt h∆°n ·ªü file ti·∫øp theo...*

