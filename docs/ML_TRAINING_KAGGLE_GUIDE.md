# üéØ H∆Ø·ªöNG D·∫™N TRAINING ML TR√äN KAGGLE - D·ªÑ HI·ªÇU

## üìë M·ª§C L·ª§C

1. [Kaggle l√† g√¨? T·∫°i sao d√πng Kaggle?](#1-kaggle-l√†-g√¨-t·∫°i-sao-d√πng-kaggle)
2. [Chu·∫©n b·ªã d·ªØ li·ªáu](#2-chu·∫©n-b·ªã-d·ªØ-li·ªáu)
3. [Setup Kaggle Notebook](#3-setup-kaggle-notebook)
4. [Upload d·ªØ li·ªáu l√™n Kaggle](#4-upload-d·ªØ-li·ªáu-l√™n-kaggle)
5. [Training Model - T·ª´ng b∆∞·ªõc chi ti·∫øt](#5-training-model---t·ª´ng-b∆∞·ªõc-chi-ti·∫øt)
6. [Download Model v·ªÅ m√°y](#6-download-model-v·ªÅ-m√°y)
7. [Troubleshooting - X·ª≠ l√Ω l·ªói](#7-troubleshooting---x·ª≠-l√Ω-l·ªói)

---

## 1. KAGGLE L√Ä G√å? T·∫†I SAO D√ôNG KAGGLE?

### 1.1. Kaggle l√† g√¨?

**Kaggle** l√† m·ªôt platform mi·ªÖn ph√≠ c·ªßa Google cho ph√©p b·∫°n:
- ‚úÖ Ch·∫°y code Python v·ªõi GPU m·∫°nh (P100, 16GB VRAM)
- ‚úÖ L∆∞u tr·ªØ dataset l·ªõn (30GB free)
- ‚úÖ Ch·∫°y notebook Jupyter tr·ª±c ti·∫øp tr√™n tr√¨nh duy·ªát
- ‚úÖ Kh√¥ng c·∫ßn c√†i ƒë·∫∑t g√¨ tr√™n m√°y t√≠nh c·ªßa b·∫°n

**So s√°nh v·ªõi c√°c platform kh√°c:**

| Platform | GPU | Storage | Th·ªùi gian | ∆Øu ƒëi·ªÉm |
|----------|-----|---------|-----------|---------|
| **Kaggle** | ‚úÖ P100 (16GB) | 30GB | 9h/session | ·ªîn ƒë·ªãnh, d·ªÖ d√πng |
| **Google Colab** | ‚úÖ T4 (16GB) | 15GB | 12h/session | T√≠ch h·ª£p Google Drive |
| **Local** | ‚ùå C·∫ßn mua | Unlimited | Unlimited | T·ªën ti·ªÅn GPU |

**üëâ Khuy·∫øn ngh·ªã: D√πng Kaggle v√¨ ·ªïn ƒë·ªãnh v√† d·ªÖ s·ª≠ d·ª•ng h∆°n Colab.**

### 1.2. T·∫°i sao c·∫ßn GPU?

**Training ML model** c·∫ßn t√≠nh to√°n r·∫•t nhi·ªÅu:
- M·ªôt model Go c√≥ th·ªÉ c√≥ h√†ng tri·ªáu tham s·ªë
- Training tr√™n CPU: **10-20 gi·ªù** cho 1 epoch
- Training tr√™n GPU: **10-20 ph√∫t** cho 1 epoch

**üëâ GPU nhanh h∆°n CPU kho·∫£ng 50-100 l·∫ßn cho deep learning!**

---

## 2. CHU·∫®N B·ªä D·ªÆ LI·ªÜU

### 2.1. D·ªØ li·ªáu c·∫ßn c√≥ g√¨?

ƒê·ªÉ train model, b·∫°n c·∫ßn:

1. **Board States** (Tr·∫°ng th√°i b√†n c·ªù)
   - Format: Tensor `[17, board_size, board_size]`
   - 17 planes = 8 l·ªãch s·ª≠ + 1 hi·ªán t·∫°i + 8 features kh√°c

2. **Labels** (Nh√£n ƒë·ªÉ train)
   - **Policy labels**: N∆∞·ªõc ƒëi ƒë√∫ng (t·ª´ professional games)
   - **Value labels**: X√°c su·∫•t th·∫Øng (0.0 - 1.0)

### 2.2. C√°ch t·∫°o d·ªØ li·ªáu

#### Option A: T·ª´ Professional Games (Khuy·∫øn ngh·ªã)

```bash
# B∆∞·ªõc 1: Download games t·ª´ KGS
python scripts/download_kgs_games.py \
  --output data/raw/kgs/ \
  --min-rank 5d \
  --max-games 5000

# B∆∞·ªõc 2: Parse SGF files th√†nh positions
python scripts/parse_sgf_colab.py \
  --input data/raw/kgs/ \
  --output data/processed/positions_9x9.pt \
  --board-size 9

# Output: data/processed/positions_9x9.pt
```

**Gi·∫£i th√≠ch:**
- `--min-rank 5d`: Ch·ªâ l·∫•y games t·ª´ rank 5 dan tr·ªü l√™n (ch·∫•t l∆∞·ª£ng cao)
- `--max-games 5000`: T·ªëi ƒëa 5000 games
- `--board-size 9`: B√†n c·ªù 9x9

#### Option B: T·ª´ Self-Play (N·∫øu kh√¥ng c√≥ professional games)

```bash
# T·∫°o games b·∫±ng AI t·ª± ƒë√°nh v·ªõi nhau
python src/ml/training/data_collector.py \
  --board-size 9 \
  --num-games 1000 \
  --output data/training/self_play_9x9.pt
```

### 2.3. Ki·ªÉm tra d·ªØ li·ªáu

Tr∆∞·ªõc khi upload, ki·ªÉm tra file c√≥ ƒë√∫ng format kh√¥ng:

```python
import torch

# Load file
data = torch.load('data/processed/positions_9x9.pt')

# Ki·ªÉm tra
print(f"S·ªë l∆∞·ª£ng positions: {len(data)}")
print(f"V√≠ d·ª• m·ªôt position:")
print(f"  - Board state shape: {data[0]['board_state'].shape}")
print(f"  - Policy shape: {data[0]['policy'].shape}")
print(f"  - Value: {data[0]['value']}")
```

**Output mong ƒë·ª£i:**
```
S·ªë l∆∞·ª£ng positions: 80000
V√≠ d·ª• m·ªôt position:
  - Board state shape: torch.Size([17, 9, 9])
  - Policy shape: torch.Size([81])
  - Value: 0.65
```

---

## 3. SETUP KAGGLE NOTEBOOK

### 3.1. T·∫°o t√†i kho·∫£n Kaggle

1. V√†o https://www.kaggle.com/
2. Click **"Sign Up"** ho·∫∑c **"Sign In"** (n·∫øu ƒë√£ c√≥ t√†i kho·∫£n)
3. ƒêƒÉng nh·∫≠p b·∫±ng Google account (d·ªÖ nh·∫•t)

### 3.2. T·∫°o Notebook m·ªõi

1. V√†o https://www.kaggle.com/code
2. Click **"New Notebook"** (g√≥c tr√™n b√™n ph·∫£i)
3. Ch·ªçn:
   - **Language**: Python
   - **Accelerator**: **GPU P100** (quan tr·ªçng!)
   - **Internet**: **On** (ƒë·ªÉ download packages)

### 3.3. C·∫•u tr√∫c th∆∞ m·ª•c Kaggle

Kaggle c√≥ c·∫•u tr√∫c th∆∞ m·ª•c nh∆∞ sau:

```
/kaggle/
‚îú‚îÄ‚îÄ input/          # N∆°i ch·ª©a datasets (ch·ªâ ƒë·ªçc)
‚îú‚îÄ‚îÄ working/        # N∆°i b·∫°n code v√† l∆∞u output (c√≥ th·ªÉ ghi)
‚îî‚îÄ‚îÄ output/         # N∆°i l∆∞u files ƒë·ªÉ download (c√≥ th·ªÉ ghi)
```

**Gi·∫£i th√≠ch:**
- `/kaggle/input/`: Dataset b·∫°n upload (read-only)
- `/kaggle/working/`: N∆°i b·∫°n code, train model (read-write)
- `/kaggle/output/`: N∆°i l∆∞u model ƒë·ªÉ download (read-write)

---

## 4. UPLOAD D·ªÆ LI·ªÜU L√äN KAGGLE

### 4.1. T·∫°o Kaggle Dataset

1. V√†o https://www.kaggle.com/datasets
2. Click **"New Dataset"**
3. Upload file `.pt` c·ªßa b·∫°n (v√≠ d·ª•: `positions_9x9.pt`)
4. ƒê·∫∑t t√™n dataset: `gogame-training-data-9x9`
5. Click **"Create"**

**L∆∞u √Ω:**
- File ph·∫£i nh·ªè h∆°n 20GB (Kaggle gi·ªõi h·∫°n)
- N·∫øu file l·ªõn, n√©n b·∫±ng `.zip` ho·∫∑c `.tar.gz` tr∆∞·ªõc

### 4.2. Add Dataset v√†o Notebook

1. Trong notebook c·ªßa b·∫°n, click **"Add data"** (g√≥c tr√™n b√™n ph·∫£i)
2. T√¨m dataset v·ª´a t·∫°o: `gogame-training-data-9x9`
3. Click **"Add"**

**Sau khi add, dataset s·∫Ω ·ªü:** `/kaggle/input/gogame-training-data-9x9/`

### 4.3. Upload Code Model

B·∫°n c√≥ 2 c√°ch:

#### C√°ch 1: Copy-paste code tr·ª±c ti·∫øp (ƒê∆°n gi·∫£n)

Copy to√†n b·ªô code t·ª´ project v√†o c√°c cells trong notebook.

#### C√°ch 2: Upload file code (Khuy·∫øn ngh·ªã)

1. N√©n folder `src/ml/` th√†nh `ml_code.zip`
2. Upload v√†o Kaggle Dataset (gi·ªëng nh∆∞ upload data)
3. Add dataset v√†o notebook
4. Gi·∫£i n√©n trong notebook:

```python
import zipfile
import os

# Gi·∫£i n√©n code
with zipfile.ZipFile('/kaggle/input/gogame-ml-code/ml_code.zip', 'r') as zip_ref:
    zip_ref.extractall('/kaggle/working/')

# Th√™m v√†o Python path
import sys
sys.path.append('/kaggle/working/src')
```

---

## 5. TRAINING MODEL - T·ª™NG B∆Ø·ªöC CHI TI·∫æT

### 5.1. Cell 1: Import v√† Setup

```python
# ============================================
# CELL 1: Import Libraries
# ============================================

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from pathlib import Path
import sys
from tqdm import tqdm

# Ki·ªÉm tra GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")
print(f"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}")

# Th√™m code v√†o path (n·∫øu upload code)
sys.path.append('/kaggle/working/src')
```

**Gi·∫£i th√≠ch:**
- `torch`: PyTorch library cho deep learning
- `device`: Ch·ªçn GPU n·∫øu c√≥, kh√¥ng th√¨ d√πng CPU
- `sys.path.append`: Th√™m th∆∞ m·ª•c code v√†o Python path

### 5.2. Cell 2: Load Model Code

```python
# ============================================
# CELL 2: Import Model Classes
# ============================================

# N·∫øu ƒë√£ upload code, import t·ª´ ƒë√≥
from ml.policy_network import PolicyNetwork, PolicyConfig
from ml.value_network import ValueNetwork, ValueConfig
from ml.features import board_to_tensor

# Ho·∫∑c ƒë·ªãnh nghƒ©a l·∫°i model (n·∫øu kh√¥ng upload code)
# (Copy code t·ª´ src/ml/policy_network.py v√† value_network.py)
```

**Gi·∫£i th√≠ch:**
- Import c√°c class model t·ª´ code ƒë√£ upload
- N·∫øu kh√¥ng upload, b·∫°n c·∫ßn copy-paste code model v√†o cell n√†y

### 5.3. Cell 3: T·∫°o Dataset Class

```python
# ============================================
# CELL 3: Dataset Class
# ============================================

class GoDataset(Dataset):
    """
    Dataset class ƒë·ªÉ load training data.
    
    M·ªói sample g·ªìm:
    - board_state: Tensor [17, 9, 9] - Tr·∫°ng th√°i b√†n c·ªù
    - policy: Tensor [81] - X√°c su·∫•t n∆∞·ªõc ƒëi (ground truth)
    - value: float - X√°c su·∫•t th·∫Øng (0.0 - 1.0)
    """
    
    def __init__(self, data_path, board_size=9):
        """
        Args:
            data_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file .pt ch·ª©a data
            board_size: K√≠ch th∆∞·ªõc b√†n c·ªù (9, 13, ho·∫∑c 19)
        """
        self.data = torch.load(data_path)
        self.board_size = board_size
        
    def __len__(self):
        """Tr·∫£ v·ªÅ s·ªë l∆∞·ª£ng samples"""
        return len(self.data)
    
    def __getitem__(self, idx):
        """
        L·∫•y m·ªôt sample t·∫°i v·ªã tr√≠ idx.
        
        Returns:
            board_state: Tensor [17, board_size, board_size]
            policy: Tensor [board_size * board_size]
            value: Tensor [1]
        """
        sample = self.data[idx]
        
        # ƒê·∫£m b·∫£o ƒë√∫ng format
        board_state = sample['board_state'].float()
        policy = sample['policy'].float()
        value = torch.tensor([sample['value']], dtype=torch.float32)
        
        return board_state, policy, value

# Test dataset
dataset_path = '/kaggle/input/gogame-training-data-9x9/positions_9x9.pt'
dataset = GoDataset(dataset_path, board_size=9)
print(f"Dataset size: {len(dataset)} samples")

# Xem m·ªôt sample
board, policy, value = dataset[0]
print(f"Board shape: {board.shape}")
print(f"Policy shape: {policy.shape}")
print(f"Value: {value.item()}")
```

**Gi·∫£i th√≠ch:**
- `GoDataset`: Class k·∫ø th·ª´a `Dataset` c·ªßa PyTorch
- `__len__()`: Tr·∫£ v·ªÅ s·ªë l∆∞·ª£ng samples
- `__getitem__()`: L·∫•y m·ªôt sample (PyTorch t·ª± ƒë·ªông g·ªçi khi training)
- `float()`: Chuy·ªÉn sang float32 (c·∫ßn cho training)

### 5.4. Cell 4: T·∫°o DataLoader

```python
# ============================================
# CELL 4: DataLoader
# ============================================

# Chia train/validation
train_size = int(0.9 * len(dataset))  # 90% train
val_size = len(dataset) - train_size  # 10% validation

train_dataset, val_dataset = torch.utils.data.random_split(
    dataset, [train_size, val_size]
)

print(f"Train samples: {len(train_dataset)}")
print(f"Val samples: {len(val_dataset)}")

# T·∫°o DataLoader
batch_size = 64  # S·ªë samples m·ªói batch

train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True,  # X√°o tr·ªôn data m·ªói epoch
    num_workers=2,  # S·ªë threads ƒë·ªÉ load data
    pin_memory=True  # TƒÉng t·ªëc transfer l√™n GPU
)

val_loader = DataLoader(
    val_dataset,
    batch_size=batch_size,
    shuffle=False,  # Kh√¥ng c·∫ßn shuffle validation
    num_workers=2,
    pin_memory=True
)

print(f"Train batches: {len(train_loader)}")
print(f"Val batches: {len(val_loader)}")
```

**Gi·∫£i th√≠ch:**
- `random_split`: Chia dataset th√†nh train (90%) v√† validation (10%)
- `batch_size`: S·ªë samples x·ª≠ l√Ω c√πng l√∫c (64 = t·ªëc ƒë·ªô v√† b·ªô nh·ªõ c√¢n b·∫±ng)
- `shuffle=True`: X√°o tr·ªôn data ƒë·ªÉ model h·ªçc t·ªët h∆°n
- `num_workers`: S·ªë threads load data song song (2-4 l√† t·ªët)
- `pin_memory=True`: TƒÉng t·ªëc transfer data l√™n GPU

### 5.5. Cell 5: Kh·ªüi t·∫°o Model

```python
# ============================================
# CELL 5: Initialize Model
# ============================================

board_size = 9
input_planes = 17  # S·ªë feature planes

# T·∫°o Policy Network
policy_config = PolicyConfig(
    board_size=board_size,
    input_planes=input_planes,
    channels=128
)
policy_net = PolicyNetwork(policy_config).to(device)

# T·∫°o Value Network
value_config = ValueConfig(
    board_size=board_size,
    input_planes=input_planes,
    channels=128
)
value_net = ValueNetwork(value_config).to(device)

# ƒê·∫øm s·ªë tham s·ªë
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

policy_params = count_parameters(policy_net)
value_params = count_parameters(value_net)

print(f"Policy Network parameters: {policy_params:,}")
print(f"Value Network parameters: {value_params:,}")
print(f"Total parameters: {policy_params + value_params:,}")

# Test forward pass
test_input = torch.randn(1, input_planes, board_size, board_size).to(device)
with torch.no_grad():
    policy_out = policy_net(test_input)
    value_out = value_net(test_input)
    
print(f"Policy output shape: {policy_out.shape}")
print(f"Value output shape: {value_out.shape}")
```

**Gi·∫£i th√≠ch:**
- `PolicyConfig` / `ValueConfig`: C·∫•u h√¨nh model (k√≠ch th∆∞·ªõc, s·ªë channels)
- `.to(device)`: Chuy·ªÉn model l√™n GPU
- `count_parameters`: ƒê·∫øm s·ªë tham s·ªë (ƒë·ªÉ bi·∫øt model l·ªõn nh·ªè th·∫ø n√†o)
- Test forward pass: Ki·ªÉm tra model ch·∫°y ƒë√∫ng kh√¥ng

### 5.6. Cell 6: Setup Training

```python
# ============================================
# CELL 6: Training Setup
# ============================================

# Loss functions
policy_loss_fn = nn.CrossEntropyLoss()  # Cho policy (classification)
value_loss_fn = nn.MSELoss()  # Cho value (regression)

# Optimizers
learning_rate = 1e-3  # 0.001
policy_optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)
value_optimizer = optim.Adam(value_net.parameters(), lr=learning_rate)

# Learning rate scheduler (gi·∫£m LR khi loss kh√¥ng gi·∫£m)
policy_scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    policy_optimizer, mode='min', factor=0.5, patience=3, verbose=True
)
value_scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    value_optimizer, mode='min', factor=0.5, patience=3, verbose=True
)

# Training parameters
num_epochs = 10  # S·ªë l·∫ßn train to√†n b·ªô dataset
save_every = 2  # L∆∞u checkpoint m·ªói 2 epochs

print("Training setup complete!")
print(f"Learning rate: {learning_rate}")
print(f"Epochs: {num_epochs}")
```

**Gi·∫£i th√≠ch:**
- `CrossEntropyLoss`: Loss cho policy (ph√¢n lo·∫°i n∆∞·ªõc ƒëi)
- `MSELoss`: Loss cho value (d·ª± ƒëo√°n x√°c su·∫•t th·∫Øng)
- `Adam`: Optimizer (t·ªët h∆°n SGD cho deep learning)
- `ReduceLROnPlateau`: T·ª± ƒë·ªông gi·∫£m learning rate khi loss kh√¥ng gi·∫£m
- `num_epochs`: S·ªë l·∫ßn train to√†n b·ªô dataset (10 = train 10 l·∫ßn)

### 5.7. Cell 7: Training Loop

```python
# ============================================
# CELL 7: Training Loop
# ============================================

def train_one_epoch(policy_net, value_net, train_loader, 
                    policy_optimizer, value_optimizer,
                    policy_loss_fn, value_loss_fn, device):
    """
    Train m·ªôt epoch.
    
    Returns:
        avg_policy_loss: Loss trung b√¨nh c·ªßa policy
        avg_value_loss: Loss trung b√¨nh c·ªßa value
    """
    policy_net.train()  # Ch·∫ø ƒë·ªô training
    value_net.train()
    
    policy_losses = []
    value_losses = []
    
    # T·∫°o progress bar
    pbar = tqdm(train_loader, desc="Training")
    
    for batch_idx, (boards, policies, values) in enumerate(pbar):
        # Chuy·ªÉn l√™n GPU
        boards = boards.to(device)
        policies = policies.to(device)
        values = values.to(device)
        
        # ===== TRAIN POLICY NETWORK =====
        policy_optimizer.zero_grad()  # Reset gradients
        
        policy_pred = policy_net(boards)  # Forward pass
        policy_loss = policy_loss_fn(policy_pred, policies.argmax(dim=1))  # T√≠nh loss
        
        policy_loss.backward()  # Backward pass (t√≠nh gradients)
        policy_optimizer.step()  # Update weights
        
        policy_losses.append(policy_loss.item())
        
        # ===== TRAIN VALUE NETWORK =====
        value_optimizer.zero_grad()
        
        value_pred = value_net(boards)
        value_loss = value_loss_fn(value_pred.squeeze(), values.squeeze())
        
        value_loss.backward()
        value_optimizer.step()
        
        value_losses.append(value_loss.item())
        
        # Update progress bar
        pbar.set_postfix({
            'policy_loss': f'{policy_loss.item():.4f}',
            'value_loss': f'{value_loss.item():.4f}'
        })
    
    return np.mean(policy_losses), np.mean(value_losses)


def validate(policy_net, value_net, val_loader,
             policy_loss_fn, value_loss_fn, device):
    """
    Validate model tr√™n validation set.
    
    Returns:
        avg_policy_loss: Loss trung b√¨nh c·ªßa policy
        avg_value_loss: Loss trung b√¨nh c·ªßa value
    """
    policy_net.eval()  # Ch·∫ø ƒë·ªô evaluation
    value_net.eval()
    
    policy_losses = []
    value_losses = []
    
    with torch.no_grad():  # Kh√¥ng t√≠nh gradients (ti·∫øt ki·ªám b·ªô nh·ªõ)
        for boards, policies, values in tqdm(val_loader, desc="Validating"):
            boards = boards.to(device)
            policies = policies.to(device)
            values = values.to(device)
            
            # Policy
            policy_pred = policy_net(boards)
            policy_loss = policy_loss_fn(policy_pred, policies.argmax(dim=1))
            policy_losses.append(policy_loss.item())
            
            # Value
            value_pred = value_net(boards)
            value_loss = value_loss_fn(value_pred.squeeze(), values.squeeze())
            value_losses.append(value_loss.item())
    
    return np.mean(policy_losses), np.mean(value_losses)


# B·∫Øt ƒë·∫ßu training
print("Starting training...")
print("=" * 50)

best_val_loss = float('inf')

for epoch in range(1, num_epochs + 1):
    print(f"\nEpoch {epoch}/{num_epochs}")
    print("-" * 50)
    
    # Train
    train_policy_loss, train_value_loss = train_one_epoch(
        policy_net, value_net, train_loader,
        policy_optimizer, value_optimizer,
        policy_loss_fn, value_loss_fn, device
    )
    
    # Validate
    val_policy_loss, val_value_loss = validate(
        policy_net, value_net, val_loader,
        policy_loss_fn, value_loss_fn, device
    )
    
    # Update learning rate
    policy_scheduler.step(val_policy_loss)
    value_scheduler.step(val_value_loss)
    
    # Print results
    print(f"Train - Policy Loss: {train_policy_loss:.4f}, Value Loss: {train_value_loss:.4f}")
    print(f"Val   - Policy Loss: {val_policy_loss:.4f}, Value Loss: {val_value_loss:.4f}")
    
    # Save checkpoint
    if epoch % save_every == 0:
        checkpoint = {
            'epoch': epoch,
            'policy_net_state_dict': policy_net.state_dict(),
            'value_net_state_dict': value_net.state_dict(),
            'policy_optimizer_state_dict': policy_optimizer.state_dict(),
            'value_optimizer_state_dict': value_optimizer.state_dict(),
            'train_policy_loss': train_policy_loss,
            'train_value_loss': train_value_loss,
            'val_policy_loss': val_policy_loss,
            'val_value_loss': val_value_loss,
        }
        
        checkpoint_path = f'/kaggle/working/checkpoint_epoch_{epoch}.pt'
        torch.save(checkpoint, checkpoint_path)
        print(f"Saved checkpoint: {checkpoint_path}")
    
    # Save best model
    current_val_loss = val_policy_loss + val_value_loss
    if current_val_loss < best_val_loss:
        best_val_loss = current_val_loss
        best_checkpoint_path = '/kaggle/working/best_model.pt'
        torch.save({
            'epoch': epoch,
            'policy_net_state_dict': policy_net.state_dict(),
            'value_net_state_dict': value_net.state_dict(),
            'val_policy_loss': val_policy_loss,
            'val_value_loss': val_value_loss,
        }, best_checkpoint_path)
        print(f"Saved best model: {best_checkpoint_path}")

print("\n" + "=" * 50)
print("Training complete!")
```

**Gi·∫£i th√≠ch chi ti·∫øt:**

1. **`train_one_epoch()`**:
   - `model.train()`: B·∫≠t ch·∫ø ƒë·ªô training (b·∫≠t dropout, batch norm update)
   - `zero_grad()`: Reset gradients v·ªÅ 0 (quan tr·ªçng!)
   - `forward()`: T√≠nh output t·ª´ input
   - `loss.backward()`: T√≠nh gradients (ƒë·∫°o h√†m)
   - `optimizer.step()`: Update weights d·ª±a tr√™n gradients

2. **`validate()`**:
   - `model.eval()`: B·∫≠t ch·∫ø ƒë·ªô evaluation (t·∫Øt dropout, freeze batch norm)
   - `torch.no_grad()`: Kh√¥ng t√≠nh gradients (ti·∫øt ki·ªám b·ªô nh·ªõ v√† nhanh h∆°n)

3. **Training loop**:
   - Train tr√™n training set
   - Validate tr√™n validation set
   - L∆∞u checkpoint m·ªói `save_every` epochs
   - L∆∞u best model (model c√≥ validation loss th·∫•p nh·∫•t)

### 5.8. Cell 8: L∆∞u Model cu·ªëi c√πng

```python
# ============================================
# CELL 8: Save Final Models
# ============================================

# L∆∞u model cu·ªëi c√πng
final_checkpoint = {
    'policy_net_state_dict': policy_net.state_dict(),
    'value_net_state_dict': value_net.state_dict(),
    'policy_config': policy_config.__dict__,
    'value_config': value_config.__dict__,
    'board_size': board_size,
}

# L∆∞u v√†o /kaggle/output ƒë·ªÉ download
torch.save(final_checkpoint, '/kaggle/output/final_model.pt')
print("Saved final model to /kaggle/output/final_model.pt")

# Copy best model
import shutil
shutil.copy('/kaggle/working/best_model.pt', '/kaggle/output/best_model.pt')
print("Saved best model to /kaggle/output/best_model.pt")
```

**Gi·∫£i th√≠ch:**
- `state_dict()`: Ch·ªâ l∆∞u weights, kh√¥ng l∆∞u to√†n b·ªô model (nh·∫π h∆°n)
- `/kaggle/output/`: Th∆∞ m·ª•c ƒë·ªÉ download files
- `/kaggle/working/`: Th∆∞ m·ª•c l√†m vi·ªác (kh√¥ng download ƒë∆∞·ª£c)

---

## 6. DOWNLOAD MODEL V·ªÄ M√ÅY

### 6.1. C√°ch 1: Download t·ª´ Kaggle UI

1. Sau khi training xong, v√†o tab **"Output"** trong notebook
2. Click v√†o file `final_model.pt` ho·∫∑c `best_model.pt`
3. Click **"Download"**

### 6.2. C√°ch 2: D√πng Kaggle API

```bash
# C√†i Kaggle API
pip install kaggle

# Setup API token (l·∫•y t·ª´ Kaggle Account Settings)
# Copy kaggle.json v√†o ~/.kaggle/

# Download file
kaggle kernels output <username>/<kernel-slug> -p ./models/
```

### 6.3. S·ª≠ d·ª•ng Model

Sau khi download, load model trong code:

```python
import torch
from src.ml.policy_network import PolicyNetwork, PolicyConfig
from src.ml.value_network import ValueNetwork, ValueConfig

# Load checkpoint
checkpoint = torch.load('models/final_model.pt', map_location='cpu')

# Kh·ªüi t·∫°o model
policy_config = PolicyConfig(**checkpoint['policy_config'])
policy_net = PolicyNetwork(policy_config)
policy_net.load_state_dict(checkpoint['policy_net_state_dict'])
policy_net.eval()

value_config = ValueConfig(**checkpoint['value_config'])
value_net = ValueNetwork(value_config)
value_net.load_state_dict(checkpoint['value_net_state_dict'])
value_net.eval()

# S·ª≠ d·ª•ng model
# ... (code inference)
```

---

## 7. TROUBLESHOOTING - X·ª¨ L√ù L·ªñI

### 7.1. L·ªói: "CUDA out of memory"

**Nguy√™n nh√¢n:** Batch size qu√° l·ªõn, kh√¥ng ƒë·ªß VRAM

**Gi·∫£i ph√°p:**
```python
# Gi·∫£m batch size
batch_size = 32  # Thay v√¨ 64

# Ho·∫∑c d√πng gradient accumulation
# (Train v·ªõi batch nh·ªè nh∆∞ng update weights nh∆∞ batch l·ªõn)
```

### 7.2. L·ªói: "File not found"

**Nguy√™n nh√¢n:** ƒê∆∞·ªùng d·∫´n dataset sai

**Gi·∫£i ph√°p:**
```python
# Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n
import os
print(os.listdir('/kaggle/input/'))

# T√¨m ƒë√∫ng t√™n dataset
# Dataset name th∆∞·ªùng c√≥ format: username/dataset-name
```

### 7.3. L·ªói: "Module not found"

**Nguy√™n nh√¢n:** Ch∆∞a import code ho·∫∑c path sai

**Gi·∫£i ph√°p:**
```python
# Ki·ªÉm tra path
import sys
print(sys.path)

# Th√™m path ƒë√∫ng
sys.path.append('/kaggle/working/src')

# Ho·∫∑c copy-paste code tr·ª±c ti·∫øp v√†o notebook
```

### 7.4. Training qu√° ch·∫≠m

**Nguy√™n nh√¢n:**
- Ch∆∞a b·∫≠t GPU
- Batch size qu√° nh·ªè
- DataLoader kh√¥ng t·ªëi ∆∞u

**Gi·∫£i ph√°p:**
```python
# Ki·ªÉm tra GPU
print(torch.cuda.is_available())
print(torch.cuda.get_device_name(0))

# TƒÉng batch size (n·∫øu ƒë·ªß VRAM)
batch_size = 128

# T·ªëi ∆∞u DataLoader
train_loader = DataLoader(
    dataset,
    batch_size=batch_size,
    num_workers=4,  # TƒÉng s·ªë workers
    pin_memory=True,
    persistent_workers=True  # Gi·ªØ workers gi·ªØa c√°c epochs
)
```

### 7.5. Loss kh√¥ng gi·∫£m

**Nguy√™n nh√¢n:**
- Learning rate qu√° cao ho·∫∑c qu√° th·∫•p
- Data ch·∫•t l∆∞·ª£ng k√©m
- Model qu√° nh·ªè ho·∫∑c qu√° l·ªõn

**Gi·∫£i ph√°p:**
```python
# Th·ª≠ learning rate kh√°c
learning_rate = 1e-4  # Thay v√¨ 1e-3

# Ho·∫∑c d√πng learning rate scheduler
scheduler = optim.lr_scheduler.CosineAnnealingLR(
    optimizer, T_max=num_epochs
)

# Ki·ªÉm tra data
print(f"Sample value range: {values.min()} - {values.max()}")
print(f"Sample policy sum: {policies.sum(dim=1)}")  # Ph·∫£i = 1
```

### 7.6. Session timeout

**Nguy√™n nh√¢n:** Kaggle gi·ªõi h·∫°n 9 gi·ªù/session

**Gi·∫£i ph√°p:**
- L∆∞u checkpoint th∆∞·ªùng xuy√™n (m·ªói epoch)
- Resume training t·ª´ checkpoint:

```python
# Load checkpoint
checkpoint = torch.load('/kaggle/working/checkpoint_epoch_5.pt')

# Resume
policy_net.load_state_dict(checkpoint['policy_net_state_dict'])
value_net.load_state_dict(checkpoint['value_net_state_dict'])
policy_optimizer.load_state_dict(checkpoint['policy_optimizer_state_dict'])
value_optimizer.load_state_dict(checkpoint['value_optimizer_state_dict'])

start_epoch = checkpoint['epoch'] + 1

# Ti·∫øp t·ª•c training t·ª´ epoch start_epoch
for epoch in range(start_epoch, num_epochs + 1):
    # ... training code
```

---

## 8. TIPS & BEST PRACTICES

### 8.1. T·ªëi ∆∞u Training

1. **Mixed Precision Training** (Nhanh h∆°n 2x):
```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

# Trong training loop
with autocast():
    policy_pred = policy_net(boards)
    policy_loss = policy_loss_fn(policy_pred, policies.argmax(dim=1))

scaler.scale(policy_loss).backward()
scaler.step(policy_optimizer)
scaler.update()
```

2. **Early Stopping** (D·ª´ng s·ªõm n·∫øu kh√¥ng c·∫£i thi·ªán):
```python
patience = 5
best_val_loss = float('inf')
patience_counter = 0

for epoch in range(num_epochs):
    # ... training ...
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print("Early stopping!")
            break
```

3. **TensorBoard Logging** (Theo d√µi training):
```python
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter('/kaggle/working/logs')

# Trong training loop
writer.add_scalar('Loss/Train_Policy', train_policy_loss, epoch)
writer.add_scalar('Loss/Val_Policy', val_policy_loss, epoch)
```

### 8.2. Ti·∫øt ki·ªám th·ªùi gian

1. **Ch·ªâ train tr√™n subset nh·ªè** ƒë·ªÉ test code tr∆∞·ªõc:
```python
# Test v·ªõi 1000 samples
train_dataset = torch.utils.data.Subset(train_dataset, range(1000))
```

2. **D√πng pre-trained model** (n·∫øu c√≥):
```python
# Load weights t·ª´ model c≈©
checkpoint = torch.load('old_model.pt')
policy_net.load_state_dict(checkpoint['policy_net_state_dict'])
```

3. **Cache data** (load nhanh h∆°n):
```python
# L∆∞u processed data
torch.save(processed_data, '/kaggle/working/cached_data.pt')
```

---

## 9. T√ìM T·∫ÆT QUY TR√åNH

```
1. Chu·∫©n b·ªã data (.pt file)
   ‚Üì
2. T·∫°o Kaggle account
   ‚Üì
3. Upload data l√™n Kaggle Dataset
   ‚Üì
4. T·∫°o Notebook m·ªõi (GPU P100)
   ‚Üì
5. Add dataset v√†o notebook
   ‚Üì
6. Copy code model v√†o notebook
   ‚Üì
7. Ch·∫°y c√°c cells training
   ‚Üì
8. Download model t·ª´ Output tab
   ‚Üì
9. S·ª≠ d·ª•ng model trong project
```

---

## 10. T√ÄI LI·ªÜU THAM KH·∫¢O

- **Kaggle Documentation**: https://www.kaggle.com/docs
- **PyTorch Tutorial**: https://pytorch.org/tutorials/
- **Kaggle Notebooks Examples**: https://www.kaggle.com/code

---

## ‚úÖ CHECKLIST

Tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu:
- [ ] C√≥ t√†i kho·∫£n Kaggle
- [ ] ƒê√£ chu·∫©n b·ªã data (.pt file)
- [ ] ƒê√£ upload data l√™n Kaggle Dataset
- [ ] ƒê√£ t·∫°o Notebook v·ªõi GPU

Trong khi training:
- [ ] ƒê√£ add dataset v√†o notebook
- [ ] ƒê√£ copy code model
- [ ] ƒê√£ ch·∫°y t·∫•t c·∫£ cells
- [ ] ƒê√£ l∆∞u checkpoint th∆∞·ªùng xuy√™n

Sau khi training:
- [ ] ƒê√£ download model v·ªÅ m√°y
- [ ] ƒê√£ test model ho·∫°t ƒë·ªông ƒë√∫ng
- [ ] ƒê√£ l∆∞u model v√†o project

---

**Ch√∫c b·∫°n training th√†nh c√¥ng! üéâ**

N·∫øu c√≥ v·∫•n ƒë·ªÅ, xem ph·∫ßn [Troubleshooting](#7-troubleshooting---x·ª≠-l√Ω-l·ªói) ho·∫∑c ƒë·ªçc th√™m:
- `docs/ML_TRAINING_COLAB_GUIDE.md` - H∆∞·ªõng d·∫´n Colab (t∆∞∆°ng t·ª±)
- `docs/ML_COMPREHENSIVE_GUIDE.md` - H∆∞·ªõng d·∫´n to√†n di·ªán

