# ğŸ·ï¸ HÆ¯á»šNG DáºªN GÃN NHÃƒN TRÃŠN COLAB

## ğŸ“‘ Má»¤C Lá»¤C

1. [Tá»•ng quan](#1-tá»•ng-quan)
2. [Setup Colab](#2-setup-colab)
3. [Upload Data](#3-upload-data)
4. [GÃ¡n NhÃ£n](#4-gÃ¡n-nhÃ£n)
5. [Download Káº¿t Quáº£](#5-download-káº¿t-quáº£)
6. [Troubleshooting](#6-troubleshooting)

---

## 1. Tá»”NG QUAN

Script `generate_labels_colab.py` Ä‘Æ°á»£c tá»‘i Æ°u cho Google Colab vá»›i:

- âœ… **Incremental Save**: Tá»± Ä‘á»™ng save chunks Ä‘á»‹nh ká»³ Ä‘á»ƒ trÃ¡nh MemoryError
- âœ… **Memory Management**: Tá»± Ä‘á»™ng detect vÃ  enable incremental save khi cáº§n
- âœ… **Error Handling**: Logging chi tiáº¿t vÃ  skip lá»—i
- âœ… **Progress Tracking**: Real-time progress vá»›i tqdm
- âœ… **Google Drive Integration**: LÆ°u trá»±c tiáº¿p vÃ o Drive

### So sÃ¡nh Local vs Colab

| TÃ­nh nÄƒng | Local | Colab |
|-----------|-------|-------|
| Multiprocessing | âœ… (8+ workers) | âš ï¸ (Háº¡n cháº¿) |
| Incremental Save | âœ… | âœ… (Quan trá»ng hÆ¡n) |
| RAM Limit | ~16GB | ~12-15GB |
| Session Timeout | âŒ | âš ï¸ (90 phÃºt free) |
| GPU | âŒ | âœ… (KhÃ´ng cáº§n cho labeling) |

**Khuyáº¿n nghá»‹**: 
- **Local**: Xá»­ lÃ½ dataset lá»›n (>500K positions) vá»›i multiprocessing
- **Colab**: Xá»­ lÃ½ dataset vá»«a (<500K positions) hoáº·c test workflow

---

## 2. SETUP COLAB

### 2.1. Táº¡o Notebook Má»›i

1. Má»Ÿ [Google Colab](https://colab.research.google.com/)
2. Táº¡o notebook má»›i: `File` â†’ `New notebook`
3. Äáº·t tÃªn: `GoGame_Labeling.ipynb`

### 2.2. Mount Google Drive

```python
# Cell 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')
```

Chá»n account vÃ  cho phÃ©p truy cáº­p Drive.

### 2.3. Táº¡o Cáº¥u TrÃºc ThÆ° Má»¥c

```python
# Cell 2: Setup directories
from pathlib import Path

WORK_DIR = Path('/content/drive/MyDrive/GoGame_ML')

# Táº¡o cÃ¡c thÆ° má»¥c cáº§n thiáº¿t
(WORK_DIR / 'processed').mkdir(parents=True, exist_ok=True)
(WORK_DIR / 'datasets').mkdir(parents=True, exist_ok=True)
(WORK_DIR / 'code').mkdir(parents=True, exist_ok=True)

print(f"âœ… Working directory: {WORK_DIR}")
```

### 2.4. Upload Scripts

**CÃ¡ch 1: Upload trá»±c tiáº¿p**

1. Upload cÃ¡c file vÃ o `GoGame_ML/code/`:
   - `generate_labels_colab.py`
   - `generate_features_colab.py`

**CÃ¡ch 2: Copy code vÃ o notebook**

Copy ná»™i dung tá»« `scripts/generate_labels_colab.py` vÃ o má»™t cell.

### 2.5. Install Dependencies

```python
# Cell 3: Install packages
!pip install torch numpy tqdm
```

---

## 3. UPLOAD DATA

### 3.1. Upload Positions File

**CÃ¡ch 1: Upload tá»« Local**

1. Upload file `.pt` vÃ o `GoGame_ML/processed/`:
   ```
   /content/drive/MyDrive/GoGame_ML/processed/
   â”œâ”€â”€ positions_19x19_2019.pt
   â”œâ”€â”€ positions_19x19_2020.pt
   â””â”€â”€ ...
   ```

**CÃ¡ch 2: Download tá»« URL (náº¿u cÃ³)**

```python
# Cell 4: Download data (náº¿u cáº§n)
import urllib.request

url = "https://your-url.com/positions_19x19_2019.pt"
output_path = WORK_DIR / 'processed' / 'positions_19x19_2019.pt'

urllib.request.urlretrieve(url, output_path)
print(f"âœ… Downloaded to {output_path}")
```

### 3.2. Verify Data

```python
# Cell 5: Verify positions file
import torch

data_path = WORK_DIR / 'processed' / 'positions_19x19_2019.pt'
data = torch.load(data_path, map_location='cpu', weights_only=False)

print(f"ğŸ“Š Data info:")
print(f"   Board size: {data['board_size']}x{data['board_size']}")
print(f"   Total positions: {len(data['positions']):,}")
print(f"   Year: {data.get('year', 'N/A')}")

# Estimate memory
estimated_mb = len(data['positions']) * 50 / 1024
print(f"   Estimated memory: ~{estimated_mb:.0f}MB")
```

---

## 4. GÃN NHÃƒN

### 4.1. Import Script

```python
# Cell 6: Import labeling script
import sys
sys.path.insert(0, str(WORK_DIR / 'code'))

from generate_labels_colab import process_dataset_file
```

### 4.2. Process Má»™t File

```python
# Cell 7: Generate labels cho má»™t file
process_dataset_file(
    input_path=str(WORK_DIR / 'processed' / 'positions_19x19_2019.pt'),
    output_path=str(WORK_DIR / 'datasets' / 'labeled_19x19_2019.pt'),
    filter_handicap=True,
    save_chunk_size=50000,  # Save má»—i 50K samples (~1.2GB)
    auto_enable_incremental=True  # Tá»± Ä‘á»™ng enable náº¿u estimated memory > 4GB
)
```

### 4.3. Process Nhiá»u Files (Theo NÄƒm)

```python
# Cell 8: Process nhiá»u nÄƒm
for year in [2019, 2020, 2021]:
    input_file = WORK_DIR / 'processed' / f'positions_19x19_{year}.pt'
    output_file = WORK_DIR / 'datasets' / f'labeled_19x19_{year}.pt'
    
    if input_file.exists():
        print(f"\nğŸ”„ Processing year {year}...")
        process_dataset_file(
            input_path=str(input_file),
            output_path=str(output_file),
            filter_handicap=True,
            save_chunk_size=50000,
            auto_enable_incremental=True
        )
    else:
        print(f"âš ï¸  Skipping year {year} (file not found)")

print("\nâœ… All years processed!")
```

### 4.4. Monitor Progress

Script sáº½ hiá»ƒn thá»‹:
- Progress bar vá»›i tá»‘c Ä‘á»™ xá»­ lÃ½ (pos/s)
- Memory usage warnings
- Chunk save notifications
- Error summary

**VÃ­ dá»¥ output:**
```
ğŸ’¡ Auto-enabling incremental save (chunk size: 50,000) to prevent MemoryError (estimated: ~15,000MB)
ğŸ“ Incremental save enabled: chunks will be saved to /content/drive/MyDrive/GoGame_ML/datasets/labeled_19x19_2019_chunks
Generating labels: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 622k/622k [2:15:30<00:00, 76.5pos/s]
ğŸ’¾ Saving chunk 1 (50,000 samples) to chunk_0001.pt
âœ… Chunk 1 saved. Memory cleared.
...
ğŸ“¦ Merging 13 chunks...
âœ… Saved merged dataset to labeled_19x19_2019.pt
```

---

## 5. DOWNLOAD Káº¾T QUáº¢

### 5.1. Verify Output

```python
# Cell 9: Verify labeled dataset
import torch

dataset_path = WORK_DIR / 'datasets' / 'labeled_19x19_2019.pt'
data = torch.load(dataset_path, map_location='cpu', weights_only=False)

print(f"ğŸ“Š Labeled dataset info:")
print(f"   Board size: {data['board_size']}x{data['board_size']}")
print(f"   Total samples: {data['total']:,}")

# Xem má»™t sample
sample = data['labeled_data'][0]
print(f"\nğŸ“ Sample structure:")
print(f"   Features shape: {sample['features'].shape}")
print(f"   Policy shape: {sample['policy'].shape}")
print(f"   Value: {sample['value']}")
```

### 5.2. Download vá» Local

**CÃ¡ch 1: Download tá»« Colab**

```python
# Cell 10: Download file
from google.colab import files

# Download labeled dataset
files.download(str(WORK_DIR / 'datasets' / 'labeled_19x19_2019.pt'))
```

**CÃ¡ch 2: Copy tá»« Drive**

Files Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o Google Drive, báº¡n cÃ³ thá»ƒ:
1. Má»Ÿ Google Drive
2. TÃ¬m file trong `GoGame_ML/datasets/`
3. Download vá» mÃ¡y

---

## 6. TROUBLESHOOTING

### 6.1. MemoryError khi Merge Chunks

**Triá»‡u chá»©ng:**
- GÃ¡n nhÃ£n xong (100%) nhÆ°ng merge chunks bá»‹ dá»«ng á»Ÿ 50%
- RAM háº¿t (12.4/12.7 GB)
- Process bá»‹ kill

**Giáº£i phÃ¡p:**

**Option 1: Skip merge vÃ  merge sau (Khuyáº¿n nghá»‹)**

```python
# GÃ¡n nhÃ£n vá»›i skip_merge=True
process_dataset_file(
    input_path=WORK_DIR / 'processed' / 'positions_19x19_2019.pt',
    output_path=WORK_DIR / 'datasets' / 'labeled_19x19_2019.pt',
    filter_handicap=True,
    save_chunk_size=50000,
    skip_merge=True  # â­ Bá» qua merge, giá»¯ chunks riÃªng
)

# Sau Ä‘Ã³, restart runtime vÃ  merge riÃªng
from merge_chunks_colab import merge_chunks_from_directory

chunks_dir = WORK_DIR / 'datasets' / 'labeled_19x19_2019_chunks'
output_path = WORK_DIR / 'datasets' / 'labeled_19x19_2019.pt'
merge_chunks_from_directory(chunks_dir, output_path)
```

**Option 2: Giáº£m chunk size**

```python
# Giáº£m chunk size Ä‘á»ƒ cÃ³ Ã­t chunks hÆ¡n
process_dataset_file(
    input_path=...,
    output_path=...,
    save_chunk_size=30000  # Giáº£m tá»« 50000 â†’ Ã­t chunks hÆ¡n
)
```

**Option 3: Restart vÃ  merge**

1. Restart runtime: `Runtime` â†’ `Restart runtime`
2. Mount Drive láº¡i
3. Merge chunks:

```python
from generate_labels_colab import merge_chunks
from pathlib import Path

chunks_dir = Path('/content/drive/MyDrive/GoGame_ML/datasets/labeled_19x19_2019_chunks')
chunk_files = sorted(chunks_dir.glob('chunk_*.pt'))

output_path = Path('/content/drive/MyDrive/GoGame_ML/datasets/labeled_19x19_2019.pt')
merge_chunks(chunk_files, output_path)
```

### 6.2. Session Timeout

**Triá»‡u chá»©ng:**
- Runtime bá»‹ disconnect sau 90 phÃºt (free tier)

**Giáº£i phÃ¡p:**
1. **Incremental save Ä‘Ã£ tá»± Ä‘á»™ng xá»­ lÃ½**: Náº¿u crash, chunks Ä‘Ã£ Ä‘Æ°á»£c save
2. **Resume tá»« chunks**: Load vÃ  merge chunks cÃ²n láº¡i

```python
# Merge chunks cÃ²n láº¡i (náº¿u crash)
from generate_labels_colab import merge_chunks
from pathlib import Path

chunks_dir = WORK_DIR / 'datasets' / 'labeled_19x19_2019_chunks'
chunk_files = sorted(chunks_dir.glob('chunk_*.pt'))

output_path = WORK_DIR / 'datasets' / 'labeled_19x19_2019.pt'
merge_chunks(chunk_files, output_path)
```

### 6.3. Import Error

**Triá»‡u chá»©ng:**
```
ModuleNotFoundError: No module named 'generate_features_colab'
```

**Giáº£i phÃ¡p:**
1. Äáº£m báº£o Ä‘Ã£ upload `generate_features_colab.py` vÃ o `code/`
2. Hoáº·c copy code vÃ o notebook

```python
# Copy code trá»±c tiáº¿p vÃ o notebook
# (Xem scripts/generate_features_colab.py)
```

### 6.4. Slow Processing

**Triá»‡u chá»©ng:**
- Tá»‘c Ä‘á»™ < 50 pos/s

**Giáº£i phÃ¡p:**
1. Colab free tier cÃ³ giá»›i háº¡n CPU
2. Xá»­ lÃ½ trÃªn local vá»›i multiprocessing sáº½ nhanh hÆ¡n
3. Hoáº·c upgrade Colab Pro

### 6.5. Chunks KhÃ´ng Merge

**Triá»‡u chá»©ng:**
- CÃ³ chunks nhÆ°ng khÃ´ng cÃ³ file merged

**Giáº£i phÃ¡p:**
```python
# Merge thá»§ cÃ´ng
from generate_labels_colab import merge_chunks
from pathlib import Path

chunks_dir = WORK_DIR / 'datasets' / 'labeled_19x19_2019_chunks'
chunk_files = sorted(chunks_dir.glob('chunk_*.pt'))

if chunk_files:
    output_path = WORK_DIR / 'datasets' / 'labeled_19x19_2019.pt'
    merge_chunks(chunk_files, output_path)
    print(f"âœ… Merged {len(chunk_files)} chunks")
else:
    print("âš ï¸  No chunks found")
```

---

## 7. BEST PRACTICES

### 7.1. Chunk Size

- **Small dataset (<100K)**: KhÃ´ng cáº§n incremental save
- **Medium (100K-500K)**: `save_chunk_size=50000`
- **Large (>500K)**: `save_chunk_size=30000` hoáº·c xá»­ lÃ½ trÃªn local

### 7.2. Batch Processing

Xá»­ lÃ½ tá»«ng nÄƒm/file riÃªng biá»‡t Ä‘á»ƒ:
- Dá»… monitor progress
- TrÃ¡nh timeout
- Dá»… resume náº¿u crash

### 7.3. Backup

- Files Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o Google Drive (tá»± Ä‘á»™ng backup)
- Giá»¯ chunks Ä‘á»ƒ cÃ³ thá»ƒ merge láº¡i náº¿u cáº§n

### 7.4. Monitoring

Theo dÃµi:
- Memory usage warnings
- Processing speed
- Chunk save frequency

---

## 8. QUICK REFERENCE

### Command Template

```python
from generate_labels_colab import process_dataset_file
from pathlib import Path

WORK_DIR = Path('/content/drive/MyDrive/GoGame_ML')

process_dataset_file(
    input_path=WORK_DIR / 'processed' / 'positions_19x19_2019.pt',
    output_path=WORK_DIR / 'datasets' / 'labeled_19x19_2019.pt',
    filter_handicap=True,
    save_chunk_size=50000,
    auto_enable_incremental=True
)
```

### Check Progress

```python
# Xem chunks Ä‘Ã£ save
chunks_dir = WORK_DIR / 'datasets' / 'labeled_19x19_2019_chunks'
chunks = sorted(chunks_dir.glob('chunk_*.pt'))
print(f"ğŸ“¦ Saved {len(chunks)} chunks")
for chunk in chunks:
    data = torch.load(chunk, map_location='cpu', weights_only=False)
    print(f"   {chunk.name}: {data['total_samples']:,} samples")
```

---

## 9. NEXT STEPS

Sau khi gÃ¡n nhÃ£n xong:

1. **Verify Dataset**: Kiá»ƒm tra sá»‘ lÆ°á»£ng samples vÃ  format
2. **Merge Years** (náº¿u cáº§n): DÃ¹ng `merge_datasets.py` Ä‘á»ƒ gá»™p nhiá»u nÄƒm
3. **Train Model**: Sá»­ dá»¥ng `train_colab.py` Ä‘á»ƒ train model

Xem thÃªm:
- `docs/ML_TRAINING_COLAB_GUIDE.md` - HÆ°á»›ng dáº«n training
- `scripts/README_COLAB_TRAINING.md` - Quick start

---

**ChÃºc báº¡n thÃ nh cÃ´ng! ğŸ‰**

