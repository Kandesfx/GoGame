# ğŸ”„ Cáº­p Nháº­t: GÃ¡n NhÃ£n TrÃªn Colab

## âœ… ÄÃ£ Cáº­p Nháº­t

Script `generate_labels_colab.py` Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t Ä‘á»ƒ sá»­ dá»¥ng **Multi-task Labels** giá»‘ng nhÆ° local script.

### CÃ¡c Thay Äá»•i

1. âœ… **Multi-task Label Generators**: ÄÃ£ tÃ­ch há»£p Ä‘áº§y Ä‘á»§
   - `ThreatLabelGenerator` - Threat detection map
   - `AttackLabelGenerator` - Attack opportunity map
   - `IntentLabelGenerator` - Intent classification (5 classes)
   - `EvaluationLabelGenerator` - Position evaluation

2. âœ… **Format Labels**: ÄÃºng theo tÃ i liá»‡u `MULTI_TASK_LABELS_IMPLEMENTATION.md`
   ```python
   {
       'features': Tensor[17, board_size, board_size],
       'labels': {
           'threat_map': Tensor[board_size, board_size],
           'attack_map': Tensor[board_size, board_size],
           'intent': {
               'type': str,  # 'territory', 'attack', 'defense', 'connection', 'cut'
               'confidence': float,
               'region': List[Tuple[int, int]]
           },
           'evaluation': {
               'win_probability': float,
               'territory_map': Tensor[board_size, board_size],
               'influence_map': Tensor[board_size, board_size]
           }
       },
       'policy': Tensor[board_size * board_size + 1],  # Backward compat
       'value': float,  # Backward compat
       'metadata': {...}
   }
   ```

3. âœ… **Metadata**: ÄÃ£ thÃªm `date_processed` vÃ  `errors` count

## ğŸ“‹ CÃ¡ch Sá»­ Dá»¥ng TrÃªn Colab

### 1. Upload Files LÃªn Colab

```python
# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Upload cÃ¡c files cáº§n thiáº¿t:
# - scripts/generate_labels_colab.py
# - scripts/label_generators.py
# - scripts/generate_features_colab.py
# - data/processed/positions_*.pt (tá»« local)
```

### 2. Generate Labels

```python
from pathlib import Path
from generate_labels_colab import process_dataset_file

WORK_DIR = Path('/content/drive/MyDrive/GoGame_ML')

# Process má»™t nÄƒm
process_dataset_file(
    input_path=WORK_DIR / 'processed' / 'positions_19x19_2019.pt',
    output_path=WORK_DIR / 'datasets' / 'labeled_19x19_2019.pt',
    filter_handicap=True,
    save_chunk_size=50000  # Quan trá»ng cho Colab RAM limit
)
```

### 3. Process Nhiá»u NÄƒm

```python
years = [2019, 2018, 2017]
board_sizes = [19, 13, 9]

for year in years:
    for board_size in board_sizes:
        input_file = WORK_DIR / 'processed' / f'positions_{board_size}x{board_size}_{year}.pt'
        output_file = WORK_DIR / 'datasets' / f'labeled_{board_size}x{board_size}_{year}.pt'
        
        if input_file.exists():
            print(f"Processing {year} - {board_size}x{board_size}...")
            process_dataset_file(
                input_path=input_file,
                output_path=output_file,
                filter_handicap=True,
                save_chunk_size=50000
            )
        else:
            print(f"Skipping {year} - {board_size}x{board_size} (file not found)")
```

## âš™ï¸ TÃ¹y Chá»n

### Incremental Save (Khuyáº¿n Nghá»‹ cho Colab)

Colab cÃ³ RAM limit (~12-15GB), nÃªn nÃªn dÃ¹ng incremental save:

```python
process_dataset_file(
    input_path=...,
    output_path=...,
    save_chunk_size=50000,  # Save má»—i 50K samples (~1.2GB)
    skip_merge=False  # True náº¿u muá»‘n giá»¯ chunks riÃªng
)
```

### Auto-enable Incremental Save

Script tá»± Ä‘á»™ng enable náº¿u estimated memory > 4GB:

```python
process_dataset_file(
    input_path=...,
    output_path=...,
    auto_enable_incremental=True  # Default: True
)
```

### Giá»¯ Chunks RiÃªng (Náº¿u RAM quÃ¡ tháº¥p)

```python
process_dataset_file(
    input_path=...,
    output_path=...,
    save_chunk_size=50000,
    skip_merge=True  # Giá»¯ chunks riÃªng, merge sau
)

# Merge sau (khi cÃ³ Ä‘á»§ RAM)
from generate_labels_colab import merge_chunks
chunk_files = sorted(WORK_DIR / 'datasets' / 'labeled_19x19_2019_chunks' / '*.pt')
merge_chunks(chunk_files, WORK_DIR / 'datasets' / 'labeled_19x19_2019.pt')
```

## ğŸ“Š So SÃ¡nh: Colab vs Local

| Feature | Colab Script | Local Script |
|---------|-------------|--------------|
| Multi-task Labels | âœ… | âœ… |
| Incremental Save | âœ… (quan trá»ng) | âœ… (optional) |
| Multiprocessing | âŒ (single-threaded) | âœ… (multiprocessing) |
| Memory Management | âœ… (auto) | âœ… (auto) |
| Error Handling | âœ… | âœ… |
| Progress Tracking | âœ… (tqdm) | âœ… (tqdm) |

**LÃ½ do khÃ´ng dÃ¹ng multiprocessing trÃªn Colab:**
- Colab cÃ³ giá»›i háº¡n sá»‘ processes
- Single-threaded Ä‘á»§ nhanh vá»›i Colab CPU
- TrÃ¡nh overhead cá»§a multiprocessing

## âš ï¸ LÆ°u Ã

1. **RAM Limit**: Colab free cÃ³ ~12GB RAM. Vá»›i dataset lá»›n, báº¯t buá»™c dÃ¹ng `save_chunk_size`.

2. **Timeout**: Colab free cÃ³ timeout ~12 giá». Vá»›i dataset ráº¥t lá»›n, cÃ³ thá»ƒ cáº§n cháº¡y nhiá»u láº§n.

3. **Google Drive**: Äáº£m báº£o cÃ³ Ä‘á»§ dung lÆ°á»£ng trÃªn Drive cho output files.

4. **Upload Files**: Cáº§n upload Ä‘áº§y Ä‘á»§:
   - `generate_labels_colab.py`
   - `label_generators.py`
   - `generate_features_colab.py`
   - `positions_*.pt` files

## ğŸ”— LiÃªn Quan

- `scripts/MULTI_TASK_LABELS_IMPLEMENTATION.md` - TÃ i liá»‡u vá» multi-task labels
- `scripts/generate_labels_local.py` - Script local (tÆ°Æ¡ng tá»±)
- `scripts/label_generators.py` - Label generators

## ğŸ“ Example Notebook

```python
# Cell 1: Setup
from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.append('/content/drive/MyDrive/GoGame_ML/scripts')

# Cell 2: Import
from pathlib import Path
from generate_labels_colab import process_dataset_file

# Cell 3: Process
WORK_DIR = Path('/content/drive/MyDrive/GoGame_ML')

process_dataset_file(
    input_path=WORK_DIR / 'processed' / 'positions_19x19_2019.pt',
    output_path=WORK_DIR / 'datasets' / 'labeled_19x19_2019.pt',
    filter_handicap=True,
    save_chunk_size=50000
)
```

