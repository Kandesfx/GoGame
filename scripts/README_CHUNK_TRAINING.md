# ğŸ“¦ HÆ¯á»šNG DáºªN TRAIN Vá»šI CHUNKS

## ğŸ¯ Táº¡i sao dÃ¹ng Chunks?

Khi dataset lá»›n (>500K samples), merge táº¥t cáº£ chunks vÃ o 1 file sáº½ gÃ¢y **MemoryError** trÃªn Colab (RAM limit ~12GB).

**Giáº£i phÃ¡p**: Train trá»±c tiáº¿p tá»« chunks mÃ  khÃ´ng cáº§n merge!

## âœ… Æ¯u Ä‘iá»ƒm

- âœ… **KhÃ´ng cáº§n merge**: Tiáº¿t kiá»‡m RAM vÃ  thá»i gian
- âœ… **Memory-efficient**: Chá»‰ cache 1 chunk táº¡i má»™t thá»i Ä‘iá»ƒm
- âœ… **Tá»± Ä‘á»™ng**: Auto-detect board_size tá»« dataset
- âœ… **TÆ°Æ¡ng thÃ­ch**: CÃ³ thá»ƒ dÃ¹ng vá»›i merged file hoáº·c chunks

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### 1. Upload Files

Upload vÃ o `GoGame_ML/code/`:
- `chunk_dataset.py` â­ (File má»›i)
- `train_colab.py` (ÄÃ£ cáº­p nháº­t)

### 2. Import vÃ  Train

```python
from pathlib import Path
from train_colab import train_model

WORK_DIR = Path('/content/drive/MyDrive/GoGame_ML')

# Train tá»« chunks
train_model(
    train_dataset_path=str(WORK_DIR / 'datasets' / 'labeled_19x19_2019_chunks'),  # Chunks directory
    val_dataset_path=None,
    board_size=None,  # Auto-detect
    batch_size=16,  # â­ Giáº£m náº¿u gáº·p RAM issues
    num_epochs=10,
    learning_rate=0.001,
    checkpoint_dir=str(WORK_DIR / 'checkpoints'),
    use_chunks=True  # â­ Enable chunks mode
)
```

### 3. Hoáº·c dÃ¹ng trá»±c tiáº¿p ChunkDataset

```python
from chunk_dataset import create_chunk_dataset
from torch.utils.data import DataLoader

# Táº¡o dataset tá»« chunks
chunks_dir = WORK_DIR / 'datasets' / 'labeled_19x19_2019_chunks'
train_dataset = create_chunk_dataset(chunks_dir, augment=True)

# Táº¡o DataLoader (tá»‘i Æ°u memory)
train_loader = DataLoader(
    train_dataset,
    batch_size=16,  # Giáº£m náº¿u cáº§n
    shuffle=True,
    num_workers=0,
    pin_memory=False,  # Táº¯t Ä‘á»ƒ giáº£m RAM
    prefetch_factor=2
)

# Train nhÆ° bÃ¬nh thÆ°á»ng
# ...
```

## âš™ï¸ Tá»‘i Æ°u Memory

### Batch Size

| RAM Available | Batch Size | Chunks |
|--------------|------------|--------|
| < 8GB | 8-12 | âœ… |
| 8-12GB | 12-16 | âœ… |
| > 12GB | 16-32 | âœ… |

### DataLoader Settings

```python
DataLoader(
    dataset,
    batch_size=16,  # â­ Giáº£m náº¿u RAM háº¿t
    shuffle=True,
    num_workers=0,  # Colab khÃ´ng support multiprocessing
    pin_memory=False,  # â­ Táº¯t Ä‘á»ƒ giáº£m RAM
    prefetch_factor=2,  # â­ Giáº£m prefetch
    persistent_workers=False
)
```

## ğŸ”§ Troubleshooting

### Váº«n bá»‹ Full RAM?

1. **Giáº£m batch_size**: 16 â†’ 8 hoáº·c 4
2. **Clear cache Ä‘á»‹nh ká»³**:
   ```python
   # Sau má»—i epoch
   if hasattr(train_dataset, 'clear_cache'):
       train_dataset.clear_cache()
       import gc
       gc.collect()
   ```
3. **Giáº£m chunk size khi táº¡o**:
   ```python
   # Khi gÃ¡n nhÃ£n
   save_chunk_size=30000  # Thay vÃ¬ 50000
   ```

### ChunkDataset not found?

```python
# Äáº£m báº£o Ä‘Ã£ upload chunk_dataset.py vÃ o code/
import sys
sys.path.insert(0, str(WORK_DIR / 'code'))
from chunk_dataset import ChunkDataset
```

## ğŸ“Š So sÃ¡nh

| Method | RAM Usage | Speed | Shuffle |
|--------|-----------|-------|---------|
| Merged File | ~15GB+ | Fast | âœ… |
| Chunks (1 cache) | ~2-3GB | Medium | âœ… |
| Chunks (no cache) | ~1GB | Slow | âš ï¸ |

## ğŸ’¡ Best Practices

1. **Dataset nhá» (<200K)**: DÃ¹ng merged file
2. **Dataset lá»›n (>200K)**: DÃ¹ng chunks
3. **Batch size**: Báº¯t Ä‘áº§u vá»›i 16, giáº£m náº¿u cáº§n
4. **Clear cache**: Sau má»—i epoch náº¿u RAM cao
5. **Monitor RAM**: Theo dÃµi trong Colab resource monitor

---

**ChÃºc báº¡n train thÃ nh cÃ´ng! ğŸ‰**

