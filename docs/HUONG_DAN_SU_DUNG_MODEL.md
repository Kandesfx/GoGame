# üìñ H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG MODEL ƒê√É TRAIN

## üìã M·ª•c L·ª•c

1. [T·ªïng Quan](#1-t·ªïng-quan)
2. [C√†i ƒê·∫∑t v√† Y√™u C·∫ßu](#2-c√†i-ƒë·∫∑t-v√†-y√™u-c·∫ßu)
3. [Load Model](#3-load-model)
4. [Chu·∫©n B·ªã D·ªØ Li·ªáu ƒê·∫ßu V√†o](#4-chu·∫©n-b·ªã-d·ªØ-li·ªáu-ƒë·∫ßu-v√†o)
5. [Th·ª±c Hi·ªán D·ª± ƒêo√°n](#5-th·ª±c-hi·ªán-d·ª±-ƒëo√°n)
6. [T√≠ch H·ª£p V√†o Game](#6-t√≠ch-h·ª£p-v√†o-game)
7. [V√≠ D·ª• Ho√†n Ch·ªânh](#7-v√≠-d·ª•-ho√†n-ch·ªânh)
8. [Troubleshooting](#8-troubleshooting)

---

## 1. T·ªïng Quan

Sau khi training xong, b·∫°n s·∫Ω c√≥ c√°c checkpoint files:
- `best_model.pt` - Model t·ªët nh·∫•t (validation loss th·∫•p nh·∫•t) ‚≠ê **Khuy·∫øn ngh·ªã d√πng**
- `final_model.pt` - Model sau epoch cu·ªëi c√πng
- `checkpoint_epoch_X.pt` - Checkpoints ƒë·ªãnh k·ª≥

Model bao g·ªìm 2 networks:
- **Policy Network**: D·ª± ƒëo√°n x√°c su·∫•t cho m·ªói n∆∞·ªõc ƒëi (move probabilities)
- **Value Network**: D·ª± ƒëo√°n x√°c su·∫•t th·∫Øng c·ªßa ng∆∞·ªùi ch∆°i hi·ªán t·∫°i (win probability)

### üìç V·ªã Tr√≠ ƒê·∫∑t File Model

**Quan tr·ªçng**: Sau khi t·∫£i file model (v√≠ d·ª•: `final_model.pt`), b·∫°n c·∫ßn ƒë·∫∑t n√≥ v√†o th∆∞ m·ª•c `checkpoints/` ·ªü **root c·ªßa project**:

```
GoGame-master/
‚îú‚îÄ‚îÄ checkpoints/              ‚Üê ƒê·∫∑t file model ·ªü ƒë√¢y
‚îÇ   ‚îú‚îÄ‚îÄ final_model.pt       ‚Üê Copy file c·ªßa b·∫°n v√†o ƒë√¢y
‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ docs/
‚îú‚îÄ‚îÄ scripts/
‚îî‚îÄ‚îÄ ...
```

**C√°c b∆∞·ªõc:**
1. T√¨m th∆∞ m·ª•c `GoGame-master` (th∆∞ m·ª•c g·ªëc c·ªßa project)
2. V√†o th∆∞ m·ª•c `checkpoints/` (n·∫øu ch∆∞a c√≥, s·∫Ω ƒë∆∞·ª£c t·∫°o t·ª± ƒë·ªông)
3. Copy file `final_model.pt` v√†o th∆∞ m·ª•c n√†y
4. ƒê∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß s·∫Ω l√†: `GoGame-master/checkpoints/final_model.pt`

**V√≠ d·ª• ƒë∆∞·ªùng d·∫´n tr√™n Windows:**
```
C:\Users\Gigabyte\OneDrive - Ho Chi Minh city University of Industry and Trade\M√°y t√≠nh\lamphuocthuan\Python\GoGame-master\checkpoints\final_model.pt
```

Sau khi ƒë·∫∑t file, b·∫°n c√≥ th·ªÉ load model b·∫±ng ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi:
```python
checkpoint_path = 'checkpoints/final_model.pt'
```

---

## 2. C√†i ƒê·∫∑t v√† Y√™u C·∫ßu

### Dependencies

```bash
pip install torch torchvision torchaudio
pip install numpy
```

### Import c·∫ßn thi·∫øt

```python
import torch
import torch.nn.functional as F
import numpy as np
from pathlib import Path

# Import model classes (t·ª´ src/ml ho·∫∑c copy v√†o project)
from policy_network import PolicyNetwork, PolicyConfig
from value_network import ValueNetwork, ValueConfig
```

---

## 3. Load Model

### 3.1. Load t·ª´ `best_model.pt` (Khuy·∫øn ngh·ªã)

```python
def load_trained_model(checkpoint_path: str, device: str = 'cpu'):
    """
    Load trained model t·ª´ checkpoint.
    
    Args:
        checkpoint_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn checkpoint file (v√≠ d·ª•: 'checkpoints/best_model.pt')
        device: 'cpu' ho·∫∑c 'cuda'
    
    Returns:
        policy_net: PolicyNetwork instance
        value_net: ValueNetwork instance
        board_size: K√≠ch th∆∞·ªõc b√†n c·ªù
    """
    # Load checkpoint
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    
    # L·∫•y config t·ª´ checkpoint
    policy_config = PolicyConfig(**checkpoint['policy_config'])
    value_config = ValueConfig(**checkpoint['value_config'])
    board_size = checkpoint['board_size']
    
    # Kh·ªüi t·∫°o models
    policy_net = PolicyNetwork(policy_config)
    value_net = ValueNetwork(value_config)
    
    # Load weights
    policy_net.load_state_dict(checkpoint['policy_net_state_dict'])
    value_net.load_state_dict(checkpoint['value_net_state_dict'])
    
    # Chuy·ªÉn sang device v√† set eval mode
    policy_net = policy_net.to(device)
    value_net = value_net.to(device)
    policy_net.eval()
    value_net.eval()
    
    return policy_net, value_net, board_size

# S·ª≠ d·ª•ng
device = 'cuda' if torch.cuda.is_available() else 'cpu'
policy_net, value_net, board_size = load_trained_model(
    'checkpoints/best_model.pt',
    device=device
)
```

### 3.2. Load t·ª´ checkpoint epoch c·ª• th·ªÉ

```python
# Load t·ª´ checkpoint epoch 5
policy_net, value_net, board_size = load_trained_model(
    'checkpoints/dataset_2019_checkpoint_epoch_5.pt',
    device='cpu'
)
```

### 3.3. L∆∞u √Ω quan tr·ªçng

- **Lu√¥n d√πng `eval()` mode**: T·∫Øt dropout v√† batch normalization trong inference
- **D√πng `torch.no_grad()`**: T·∫Øt gradient computation ƒë·ªÉ ti·∫øt ki·ªám memory v√† tƒÉng t·ªëc
- **`map_location`**: D√πng `'cpu'` n·∫øu load tr√™n CPU, ho·∫∑c `'cuda:0'` n·∫øu load tr√™n GPU

---

## 4. Chu·∫©n B·ªã D·ªØ Li·ªáu ƒê·∫ßu V√†o

Model y√™u c·∫ßu input l√† **17-plane features** v·ªõi shape `[1, 17, board_size, board_size]`.

### 4.1. 17-Plane Features Format

| Plane | M√¥ t·∫£ |
|-------|-------|
| 0 | Qu√¢n c·ªù c·ªßa ng∆∞·ªùi ch∆°i hi·ªán t·∫°i |
| 1 | Qu√¢n c·ªù c·ªßa ƒë·ªëi th·ªß |
| 2 | Qu√¢n c·ªù c·ªßa ng∆∞·ªùi ch∆°i hi·ªán t·∫°i c√≥ 1 liberty |
| 3 | Qu√¢n c·ªù c·ªßa ƒë·ªëi th·ªß c√≥ 1 liberty |
| 4 | Qu√¢n c·ªù c·ªßa ng∆∞·ªùi ch∆°i hi·ªán t·∫°i c√≥ 2 liberties |
| 5 | Qu√¢n c·ªù c·ªßa ƒë·ªëi th·ªß c√≥ 2 liberties |
| 6 | Qu√¢n c·ªù c·ªßa ng∆∞·ªùi ch∆°i hi·ªán t·∫°i c√≥ 3+ liberties |
| 7 | Qu√¢n c·ªù c·ªßa ƒë·ªëi th·ªß c√≥ 3+ liberties |
| 8-15 | L·ªãch s·ª≠ n∆∞·ªõc ƒëi (4 n∆∞·ªõc g·∫ßn nh·∫•t, m·ªói n∆∞·ªõc = 2 planes) |
| 16 | Ch·ªâ s·ªë l∆∞·ª£t ch∆°i (1.0 n·∫øu Black, 0.0 n·∫øu White) |

### 4.2. H√†m t·∫°o 17-plane features

```python
def get_liberties_simple(board_state: np.ndarray, x: int, y: int, board_size: int) -> int:
    """T√≠nh s·ªë liberties c·ªßa m·ªôt qu√¢n c·ªù t·∫°i (x, y)."""
    if board_state[y, x] == 0:
        return 0
    
    color = board_state[y, x]
    liberties = 0
    
    # Check 4 neighbors
    for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:
        nx, ny = x + dx, y + dy
        if 0 <= nx < board_size and 0 <= ny < board_size:
            if board_state[ny, nx] == 0:
                liberties += 1
    
    return liberties


def board_to_features_17_planes(
    board_state: np.ndarray,
    current_player: str,
    move_history: list = None,
    board_size: int = 9
) -> torch.Tensor:
    """
    Convert board state th√†nh 17-plane tensor.
    
    Args:
        board_state: numpy array [board_size, board_size]
                    0 = empty, 1 = black, 2 = white
        current_player: 'B' ho·∫∑c 'W'
        move_history: List of (x, y) tuples cho last 4 moves
        board_size: K√≠ch th∆∞·ªõc b√†n c·ªù
    
    Returns:
        Tensor [17, board_size, board_size]
    """
    features = torch.zeros((17, board_size, board_size), dtype=torch.float32)
    
    # Plane 0: Current player stones
    # Plane 1: Opponent stones
    if current_player == 'B':
        features[0] = torch.from_numpy((board_state == 1).astype(np.float32))
        features[1] = torch.from_numpy((board_state == 2).astype(np.float32))
    else:  # White
        features[0] = torch.from_numpy((board_state == 2).astype(np.float32))
        features[1] = torch.from_numpy((board_state == 1).astype(np.float32))
    
    # Plane 2-7: Liberty counts
    for y in range(board_size):
        for x in range(board_size):
            if board_state[y, x] == 0:
                continue
            
            # Determine if this is current player's stone
            is_current = (
                (current_player == 'B' and board_state[y, x] == 1) or
                (current_player == 'W' and board_state[y, x] == 2)
            )
            
            if is_current:
                liberties = get_liberties_simple(board_state, x, y, board_size)
                if liberties == 1:
                    features[2, y, x] = 1.0
                elif liberties == 2:
                    features[4, y, x] = 1.0
                elif liberties >= 3:
                    features[6, y, x] = 1.0
            else:
                # Opponent stones
                liberties = get_liberties_simple(board_state, x, y, board_size)
                if liberties == 1:
                    features[3, y, x] = 1.0
                elif liberties == 2:
                    features[5, y, x] = 1.0
                elif liberties >= 3:
                    features[7, y, x] = 1.0
    
    # Plane 8-15: Move history (last 4 moves)
    if move_history is None:
        move_history = []
    
    # Ch·ªâ l·∫•y 4 n∆∞·ªõc g·∫ßn nh·∫•t
    move_history = move_history[-4:]
    
    for i, (mx, my) in enumerate(move_history):
        if i >= 4:
            break
        # M·ªói move = 2 planes (x v√† y)
        plane_x = 8 + i * 2
        plane_y = 9 + i * 2
        if 0 <= mx < board_size and 0 <= my < board_size:
            features[plane_x, my, mx] = 1.0
            features[plane_y, my, mx] = 1.0
    
    # Plane 16: Turn indicator
    features[16].fill_(1.0 if current_player == 'B' else 0.0)
    
    return features
```

### 4.3. V√≠ d·ª• t·∫°o features t·ª´ board state

```python
# Gi·∫£ s·ª≠ b·∫°n c√≥ board state t·ª´ game
board_state = np.array([
    [0, 0, 1, 0, 0],
    [0, 1, 2, 1, 0],
    [1, 2, 0, 2, 1],
    [0, 1, 2, 1, 0],
    [0, 0, 1, 0, 0]
])  # 5x5 board, 1=black, 2=white, 0=empty

current_player = 'B'  # Black's turn
move_history = [(2, 2), (1, 1)]  # Last 2 moves

# T·∫°o features
features = board_to_features_17_planes(
    board_state=board_state,
    current_player=current_player,
    move_history=move_history,
    board_size=5
)

# Th√™m batch dimension: [1, 17, board_size, board_size]
features = features.unsqueeze(0)  # Shape: [1, 17, 5, 5]
```

---

## 5. Th·ª±c Hi·ªán D·ª± ƒêo√°n

### 5.1. D·ª± ƒëo√°n Policy (Move Probabilities)

```python
def predict_move(policy_net, features, board_size):
    """
    D·ª± ƒëo√°n x√°c su·∫•t cho m·ªói n∆∞·ªõc ƒëi.
    
    Args:
        policy_net: PolicyNetwork instance
        features: Tensor [1, 17, board_size, board_size]
        board_size: K√≠ch th∆∞·ªõc b√†n c·ªù
    
    Returns:
        policy_probs: Tensor [board_size * board_size] - x√°c su·∫•t cho m·ªói move
        best_move: (x, y) - n∆∞·ªõc ƒëi c√≥ x√°c su·∫•t cao nh·∫•t
    """
    policy_net.eval()
    
    with torch.no_grad():
        # Forward pass
        policy_logits = policy_net(features)  # Shape: [1, board_size * board_size]
        
        # Convert logits to probabilities
        policy_probs = torch.exp(policy_logits[0])  # Shape: [board_size * board_size]
    
    # T√¨m n∆∞·ªõc ƒëi t·ªët nh·∫•t
    best_move_idx = torch.argmax(policy_probs).item()
    best_move_y = best_move_idx // board_size
    best_move_x = best_move_idx % board_size
    
    return policy_probs, (best_move_x, best_move_y)

# S·ª≠ d·ª•ng
policy_probs, best_move = predict_move(policy_net, features, board_size)
print(f"Best move: {best_move}")
print(f"Probability: {policy_probs[best_move[1] * board_size + best_move[0]]:.4f}")
```

### 5.2. D·ª± ƒëo√°n Value (Win Probability)

```python
def predict_value(value_net, features):
    """
    D·ª± ƒëo√°n x√°c su·∫•t th·∫Øng c·ªßa ng∆∞·ªùi ch∆°i hi·ªán t·∫°i.
    
    Args:
        value_net: ValueNetwork instance
        features: Tensor [1, 17, board_size, board_size]
    
    Returns:
        win_probability: float trong kho·∫£ng [0, 1]
    """
    value_net.eval()
    
    with torch.no_grad():
        value_pred = value_net(features)  # Shape: [1, 1]
        win_prob = value_pred[0, 0].item()
    
    return win_prob

# S·ª≠ d·ª•ng
win_prob = predict_value(value_net, features)
print(f"Win probability: {win_prob:.4f} ({win_prob * 100:.2f}%)")
```

### 5.3. D·ª± ƒëo√°n k·∫øt h·ª£p (Policy + Value)

```python
def predict(policy_net, value_net, features, board_size):
    """
    D·ª± ƒëo√°n c·∫£ policy v√† value c√πng l√∫c.
    
    Returns:
        policy_probs: Tensor [board_size * board_size]
        best_move: (x, y)
        win_prob: float
    """
    policy_net.eval()
    value_net.eval()
    
    with torch.no_grad():
        # Policy prediction
        policy_logits = policy_net(features)
        policy_probs = torch.exp(policy_logits[0])
        
        # Value prediction
        value_pred = value_net(features)
        win_prob = value_pred[0, 0].item()
    
    # Best move
    best_move_idx = torch.argmax(policy_probs).item()
    best_move_y = best_move_idx // board_size
    best_move_x = best_move_idx % board_size
    
    return policy_probs, (best_move_x, best_move_y), win_prob

# S·ª≠ d·ª•ng
policy_probs, best_move, win_prob = predict(policy_net, value_net, features, board_size)
print(f"Best move: {best_move}")
print(f"Win probability: {win_prob:.4f}")
```

---

## 6. T√≠ch H·ª£p V√†o Game

### 6.1. Class wrapper cho d·ªÖ s·ª≠ d·ª•ng

```python
class GoAIModel:
    """
    Wrapper class ƒë·ªÉ s·ª≠ d·ª•ng trained model trong game.
    """
    
    def __init__(self, checkpoint_path: str, device: str = 'cpu'):
        """
        Args:
            checkpoint_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn checkpoint file
            device: 'cpu' ho·∫∑c 'cuda'
        """
        self.device = torch.device(device)
        self.policy_net, self.value_net, self.board_size = load_trained_model(
            checkpoint_path, device=device
        )
    
    def predict_move(self, board_state: np.ndarray, current_player: str, 
                     move_history: list = None) -> tuple:
        """
        Predict move t·ª´ board state.
        
        Args:
            board_state: numpy array [board_size, board_size]
                        0 = empty, 1 = black, 2 = white
            current_player: 'B' ho·∫∑c 'W'
            move_history: List of (x, y) tuples cho last 4 moves
        
        Returns:
            best_move: (x, y) - n∆∞·ªõc ƒëi t·ªët nh·∫•t
            policy_probs: numpy array [board_size * board_size]
            win_prob: float - x√°c su·∫•t th·∫Øng
        """
        # T·∫°o features
        features = board_to_features_17_planes(
            board_state, current_player, move_history, self.board_size
        )
        features = features.unsqueeze(0).to(self.device)  # [1, 17, board_size, board_size]
        
        # Predict
        policy_probs, best_move, win_prob = predict(
            self.policy_net, self.value_net, features, self.board_size
        )
        
        # Convert to numpy
        policy_probs_np = policy_probs.cpu().numpy()
        
        return best_move, policy_probs_np, win_prob
    
    def get_top_moves(self, board_state: np.ndarray, current_player: str,
                      move_history: list = None, top_k: int = 5) -> list:
        """
        L·∫•y top K n∆∞·ªõc ƒëi t·ªët nh·∫•t.
        
        Returns:
            List of tuples: [(x, y, probability), ...]
        """
        _, policy_probs, _ = self.predict_move(board_state, current_player, move_history)
        
        # Get top K indices
        top_indices = np.argsort(policy_probs)[-top_k:][::-1]
        
        top_moves = []
        for idx in top_indices:
            y = idx // self.board_size
            x = idx % self.board_size
            prob = policy_probs[idx]
            top_moves.append((x, y, prob))
        
        return top_moves

# S·ª≠ d·ª•ng trong game
model = GoAIModel('checkpoints/best_model.pt', device='cpu')

# Trong game loop
board_state = get_current_board_state()  # H√†m c·ªßa b·∫°n
current_player = 'B'
move_history = get_recent_moves()  # H√†m c·ªßa b·∫°n

# Predict move
best_move, policy_probs, win_prob = model.predict_move(
    board_state, current_player, move_history
)

# L·∫•y top 5 moves
top_moves = model.get_top_moves(board_state, current_player, move_history, top_k=5)
for x, y, prob in top_moves:
    print(f"Move ({x}, {y}): {prob:.4f}")
```

### 6.2. T√≠ch h·ª£p v·ªõi game engine

```python
# V√≠ d·ª• t√≠ch h·ª£p v·ªõi game engine
class GameEngine:
    def __init__(self):
        self.model = GoAIModel('checkpoints/best_model.pt', device='cpu')
        self.board = initialize_board()
        self.current_player = 'B'
        self.move_history = []
    
    def ai_move(self):
        """AI th·ª±c hi·ªán n∆∞·ªõc ƒëi d·ª±a tr√™n model."""
        # L·∫•y board state
        board_state = self.get_board_state()
        
        # Predict
        best_move, _, win_prob = self.model.predict_move(
            board_state, self.current_player, self.move_history
        )
        
        # Th·ª±c hi·ªán move
        x, y = best_move
        if self.is_valid_move(x, y):
            self.make_move(x, y)
            self.move_history.append((x, y))
            self.current_player = 'W' if self.current_player == 'B' else 'B'
            return True
        
        return False
    
    def get_ai_suggestion(self):
        """L·∫•y g·ª£i √Ω n∆∞·ªõc ƒëi t·ª´ AI (kh√¥ng th·ª±c hi·ªán move)."""
        board_state = self.get_board_state()
        best_move, policy_probs, win_prob = self.model.predict_move(
            board_state, self.current_player, self.move_history
        )
        
        return {
            'best_move': best_move,
            'win_probability': win_prob,
            'top_moves': self.model.get_top_moves(
                board_state, self.current_player, self.move_history, top_k=5
            )
        }
```

---

## 7. V√≠ D·ª• Ho√†n Ch·ªânh

### 7.1. Script ƒë∆°n gi·∫£n ƒë·ªÉ test model

```python
import torch
import numpy as np
from pathlib import Path

# Import c√°c h√†m ƒë√£ ƒë·ªãnh nghƒ©a ·ªü tr√™n
from load_model import load_trained_model
from features import board_to_features_17_planes
from predict import predict

def test_model():
    """Test model v·ªõi board state m·∫´u."""
    
    # Load model
    print("Loading model...")
    checkpoint_path = 'checkpoints/best_model.pt'
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    policy_net, value_net, board_size = load_trained_model(
        checkpoint_path, device=device
    )
    print(f"‚úÖ Model loaded! Board size: {board_size}")
    
    # T·∫°o board state m·∫´u
    board_state = np.zeros((board_size, board_size), dtype=np.int32)
    # Th√™m m·ªôt s·ªë qu√¢n c·ªù m·∫´u
    board_state[3, 3] = 1  # Black
    board_state[3, 4] = 2  # White
    board_state[4, 3] = 2  # White
    board_state[4, 4] = 1  # Black
    
    current_player = 'B'
    move_history = [(3, 3), (3, 4)]
    
    # T·∫°o features
    features = board_to_features_17_planes(
        board_state, current_player, move_history, board_size
    )
    features = features.unsqueeze(0).to(device)
    
    # Predict
    print("\nPredicting...")
    policy_probs, best_move, win_prob = predict(
        policy_net, value_net, features, board_size
    )
    
    print(f"\nüìä Results:")
    print(f"  Best move: {best_move}")
    print(f"  Win probability: {win_prob:.4f} ({win_prob * 100:.2f}%)")
    print(f"  Top 5 moves:")
    
    # Top 5 moves
    top_indices = torch.argsort(policy_probs, descending=True)[:5]
    for i, idx in enumerate(top_indices):
        y = idx.item() // board_size
        x = idx.item() % board_size
        prob = policy_probs[idx].item()
        print(f"    {i+1}. ({x}, {y}): {prob:.4f}")

if __name__ == '__main__':
    test_model()
```

### 7.2. Batch prediction (nhi·ªÅu positions c√πng l√∫c)

```python
def batch_predict(policy_net, value_net, features_batch, board_size):
    """
    Predict cho nhi·ªÅu positions c√πng l√∫c (batch).
    
    Args:
        features_batch: Tensor [batch_size, 17, board_size, board_size]
    
    Returns:
        policy_probs_batch: Tensor [batch_size, board_size * board_size]
        value_batch: Tensor [batch_size, 1]
    """
    policy_net.eval()
    value_net.eval()
    
    with torch.no_grad():
        policy_logits = policy_net(features_batch)
        policy_probs = torch.exp(policy_logits)
        
        value_pred = value_net(features_batch)
    
    return policy_probs, value_pred

# S·ª≠ d·ª•ng
batch_size = 32
features_batch = torch.randn(batch_size, 17, board_size, board_size)

policy_batch, value_batch = batch_predict(
    policy_net, value_net, features_batch, board_size
)

print(f"Policy shape: {policy_batch.shape}")  # [32, board_size * board_size]
print(f"Value shape: {value_batch.shape}")  # [32, 1]
```

---

## 8. Troubleshooting

### 8.1. L·ªói: "File not found"

```python
# Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n
checkpoint_path = Path('checkpoints/best_model.pt')
if not checkpoint_path.exists():
    print(f"‚ùå Checkpoint not found: {checkpoint_path}")
    print(f"   Current directory: {Path.cwd()}")
    print(f"   Available files: {list(Path('checkpoints').glob('*.pt'))}")
```

### 8.2. L·ªói: "KeyError: 'policy_config'"

Checkpoint format c√≥ th·ªÉ kh√°c nhau. Ki·ªÉm tra keys trong checkpoint:

```python
checkpoint = torch.load('checkpoints/best_model.pt', map_location='cpu')
print("Checkpoint keys:", checkpoint.keys())

# N·∫øu kh√¥ng c√≥ 'policy_config', c√≥ th·ªÉ c·∫ßn load th·ªß c√¥ng:
if 'policy_config' not in checkpoint:
    # Th·ª≠ load v·ªõi format c≈©
    policy_config = PolicyConfig(
        board_size=checkpoint.get('board_size', 9),
        input_planes=17,
        channels=128  # ho·∫∑c t·ª´ checkpoint n·∫øu c√≥
    )
```

### 8.3. L·ªói: "Shape mismatch"

ƒê·∫£m b·∫£o features c√≥ ƒë√∫ng shape:

```python
# Ki·ªÉm tra shape
print(f"Features shape: {features.shape}")
print(f"Expected: [1, 17, {board_size}, {board_size}]")

# N·∫øu thi·∫øu batch dimension
if features.dim() == 3:
    features = features.unsqueeze(0)
```

### 8.4. Model ch·∫°y ch·∫≠m

T·ªëi ∆∞u t·ªëc ƒë·ªô:

```python
# 1. D√πng GPU n·∫øu c√≥
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# 2. D√πng torch.jit.script ho·∫∑c torch.compile (PyTorch 2.0+)
if hasattr(torch, 'compile'):
    policy_net = torch.compile(policy_net)
    value_net = torch.compile(value_net)

# 3. Batch predictions thay v√¨ t·ª´ng c√°i m·ªôt
# Thay v√¨ predict 100 l·∫ßn ri√™ng l·∫ª, gom l·∫°i th√†nh 1 batch

# 4. D√πng half precision (FP16) n·∫øu GPU h·ªó tr·ª£
if device.type == 'cuda':
    policy_net = policy_net.half()
    value_net = value_net.half()
    features = features.half()
```

### 8.5. Memory issues

```python
# 1. Clear cache sau m·ªói prediction
torch.cuda.empty_cache()

# 2. D√πng CPU n·∫øu GPU h·∫øt memory
device = 'cpu'

# 3. Gi·∫£m batch size
batch_size = 1  # Thay v√¨ 32
```

---

## üìö T√†i Li·ªáu Tham Kh·∫£o

- **Training guide**: `scripts/README_COLAB_TRAINING.md`
- **Model architecture**: `src/ml/policy_network.py`, `src/ml/value_network.py`
- **Feature generation**: `scripts/generate_features_colab.py`
- **Comprehensive ML guide**: `docs/ML_COMPREHENSIVE_GUIDE.md`

---

## üí° Tips v√† Best Practices

1. **Lu√¥n d√πng `best_model.pt`**: Model n√†y c√≥ validation loss th·∫•p nh·∫•t
2. **Set `eval()` mode**: Quan tr·ªçng ƒë·ªÉ t·∫Øt dropout v√† batch norm
3. **D√πng `torch.no_grad()`**: T·∫Øt gradient ƒë·ªÉ ti·∫øt ki·ªám memory
4. **Batch predictions**: Gom nhi·ªÅu predictions l·∫°i ƒë·ªÉ tƒÉng t·ªëc
5. **Cache features**: N·∫øu c√πng board state, cache features ƒë·ªÉ tr√°nh t√≠nh l·∫°i
6. **Validate moves**: Lu√¥n ki·ªÉm tra move c√≥ h·ª£p l·ªá tr∆∞·ªõc khi th·ª±c hi·ªán
7. **Monitor performance**: ƒêo th·ªùi gian inference ƒë·ªÉ t·ªëi ∆∞u

---

## ‚úÖ Checklist Tr∆∞·ªõc Khi S·ª≠ D·ª•ng

- [ ] Model ƒë√£ ƒë∆∞·ª£c train v√† c√≥ checkpoint file
- [ ] ƒê√£ c√†i ƒë·∫∑t ƒë·∫ßy ƒë·ªß dependencies (torch, numpy)
- [ ] ƒê√£ import ƒë√∫ng c√°c class (PolicyNetwork, ValueNetwork)
- [ ] Features c√≥ ƒë√∫ng shape [1, 17, board_size, board_size]
- [ ] Model ƒë√£ ƒë∆∞·ª£c set sang `eval()` mode
- [ ] ƒê√£ test v·ªõi board state m·∫´u tr∆∞·ªõc khi t√≠ch h·ª£p v√†o game

---
<!-- test load mode -->
python scripts/test_model_in_game.py

**Ch√∫c b·∫°n s·ª≠ d·ª•ng model th√†nh c√¥ng! üéâ**

