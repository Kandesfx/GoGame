# ğŸ“¦ HÆ¯á»šNG DáºªN TRAIN Vá»šI CHUNKS

## ğŸ¯ Táº¡i sao dÃ¹ng Chunks?

Khi dataset lá»›n (>500K samples), merge táº¥t cáº£ chunks vÃ o 1 file sáº½ gÃ¢y **MemoryError** trÃªn Colab (RAM limit ~12GB).

**Giáº£i phÃ¡p**: Train trá»±c tiáº¿p tá»« chunks mÃ  khÃ´ng cáº§n merge!

## âœ… Æ¯u Ä‘iá»ƒm

- âœ… **KhÃ´ng cáº§n merge**: Tiáº¿t kiá»‡m RAM vÃ  thá»i gian
- âœ… **Memory-efficient**: Chá»‰ cache 1 chunk táº¡i má»™t thá»i Ä‘iá»ƒm
- âœ… **Tá»± Ä‘á»™ng**: Auto-detect board_size tá»« dataset
- âœ… **TÆ°Æ¡ng thÃ­ch**: CÃ³ thá»ƒ dÃ¹ng vá»›i merged file hoáº·c chunks

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### 1. Upload Files

Upload vÃ o `GoGame_ML/code/`:
- `chunk_dataset.py` â­ (File má»›i)
- `train_colab.py` (ÄÃ£ cáº­p nháº­t)

### 2. Import vÃ  Train

```python
from pathlib import Path
from train_colab import train_model

WORK_DIR = Path('/content/drive/MyDrive/GoGame_ML')

# Train tá»« chunks
train_model(
    train_dataset_path=str(WORK_DIR / 'datasets' / 'labeled_19x19_2019_chunks'),  # Chunks directory
    val_dataset_path=None,
    board_size=None,  # Auto-detect
    batch_size=16,  # â­ Giáº£m náº¿u gáº·p RAM issues
    num_epochs=10,
    learning_rate=0.001,
    checkpoint_dir=str(WORK_DIR / 'checkpoints'),
    use_chunks=True  # â­ Enable chunks mode
)
```

### 3. Hoáº·c dÃ¹ng trá»±c tiáº¿p ChunkDataset

```python
from chunk_dataset import create_chunk_dataset
from torch.utils.data import DataLoader

# Táº¡o dataset tá»« chunks
chunks_dir = WORK_DIR / 'datasets' / 'labeled_19x19_2019_chunks'
train_dataset = create_chunk_dataset(chunks_dir, augment=True)

# Táº¡o DataLoader (tá»‘i Æ°u memory)
train_loader = DataLoader(
    train_dataset,
    batch_size=16,  # Giáº£m náº¿u cáº§n
    shuffle=True,
    num_workers=0,
    pin_memory=False,  # Táº¯t Ä‘á»ƒ giáº£m RAM
    prefetch_factor=2
)

# Train nhÆ° bÃ¬nh thÆ°á»ng
# ...
```

## âš™ï¸ Tá»‘i Æ°u Memory

### Batch Size

| RAM Available | Batch Size | Chunks |
|--------------|------------|--------|
| < 8GB | 8-12 | âœ… |
| 8-12GB | 12-16 | âœ… |
| > 12GB | 16-32 | âœ… |

### DataLoader Settings

```python
DataLoader(
    dataset,
    batch_size=16,  # â­ Giáº£m náº¿u RAM háº¿t
    shuffle=True,
    num_workers=0,  # Colab khÃ´ng support multiprocessing
    pin_memory=False,  # â­ Táº¯t Ä‘á»ƒ giáº£m RAM
    prefetch_factor=2,  # â­ Giáº£m prefetch
    persistent_workers=False
)
```

## ğŸ”§ Troubleshooting

### Váº«n bá»‹ Full RAM?

1. **Giáº£m batch_size**: 16 â†’ 8 hoáº·c 4
2. **Clear cache Ä‘á»‹nh ká»³**:
   ```python
   # Sau má»—i epoch
   if hasattr(train_dataset, 'clear_cache'):
       train_dataset.clear_cache()
       import gc
       gc.collect()
   ```
3. **Giáº£m chunk size khi táº¡o**:
   ```python
   # Khi gÃ¡n nhÃ£n
   save_chunk_size=30000  # Thay vÃ¬ 50000
   ```

### ChunkDataset not found?

```python
# Äáº£m báº£o Ä‘Ã£ upload chunk_dataset.py vÃ o code/
import sys
sys.path.insert(0, str(WORK_DIR / 'code'))
from chunk_dataset import ChunkDataset
```

## ğŸ“Š So sÃ¡nh

| Method | RAM Usage | Speed | Shuffle |
|--------|-----------|-------|---------|
| Merged File | ~15GB+ | Fast | âœ… |
| Chunks (1 cache) | ~2-3GB | Medium | âœ… |
| Chunks (no cache) | ~1GB | Slow | âš ï¸ |

## ğŸ’¡ Best Practices

1. **Dataset nhá» (<200K)**: DÃ¹ng merged file
2. **Dataset lá»›n (>200K)**: DÃ¹ng chunks
3. **Batch size**: Báº¯t Ä‘áº§u vá»›i 16, giáº£m náº¿u cáº§n
4. **Clear cache**: Sau má»—i epoch náº¿u RAM cao
5. **Monitor RAM**: Theo dÃµi trong Colab resource monitor

---
## â›‘ Backup káº¿t quáº£ training trÃªn Colab (trÃ¡nh máº¥t file khi reset)

Ngay cáº£ khi báº¡n train trá»±c tiáº¿p trÃªn Google Drive, nÃªn **backup Ä‘á»‹nh ká»³** Ä‘á»ƒ trÃ¡nh máº¥t file khi Colab bá»‹ disconnect/reset.

### 1ï¸âƒ£ ThÃªm hÃ m backup vÃ o notebook Colab

ThÃªm 1 cell trong notebook:

```python
import shutil
import datetime
from pathlib import Path

WORK_DIR = Path('/content/drive/MyDrive/GoGame_ML')  # Giá»‘ng pháº§n train
BACKUP_ROOT = WORK_DIR / 'backups'

def backup_training_results(
    src_dirs=('checkpoints', 'logs', 'outputs'),
    extra_paths=()
):
    """Táº¡o 1 báº£n backup toÃ n bá»™ káº¿t quáº£ train vÃ o Google Drive."""
    BACKUP_ROOT.mkdir(exist_ok=True)

    ts = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    backup_dir = BACKUP_ROOT / f'backup_{ts}'
    backup_dir.mkdir()

    print(f"ğŸ“¦ Äang táº¡o backup táº¡i: {backup_dir}")

    # Copy cÃ¡c thÆ° má»¥c chuáº©n
    for name in src_dirs:
        src = WORK_DIR / name
        dst = backup_dir / name
        if src.exists():
            print(f"  âœ Copy dir: {src} -> {dst}")
            shutil.copytree(src, dst)
        else:
            print(f"  âš ï¸ Bá» qua (khÃ´ng tá»“n táº¡i): {src}")

    # Copy thÃªm file/thÆ° má»¥c khÃ¡c náº¿u cáº§n
    for p in extra_paths:
        p = Path(p)
        if not p.exists():
            print(f"  âš ï¸ Bá» qua (khÃ´ng tá»“n táº¡i): {p}")
            continue
        dst = backup_dir / p.name
        if p.is_dir():
            print(f"  âœ Copy dir: {p} -> {dst}")
            shutil.copytree(p, dst)
        else:
            print(f"  âœ Copy file: {p} -> {dst}")
            shutil.copy2(p, dst)

    print("âœ… Backup hoÃ n thÃ nh!")
    return backup_dir
```

### 2ï¸âƒ£ Gá»i backup trong lÃºc train

- **Äá»‹nh ká»³ sau vÃ i epoch** hoáº·c trÆ°á»›c khi dá»«ng notebook, cháº¡y:

```python
backup_training_results()
```

- CÃ¡c báº£n backup sáº½ náº±m á»Ÿ:

```text
GoGame_ML/backups/backup_YYYYMMDD_HHMMSS/
```

### 3ï¸âƒ£ (Tuá»³ chá»n) Táº¡o file ZIP Ä‘á»ƒ táº£i vá» mÃ¡y

```python
from google.colab import files

def backup_and_download():
    backup_dir = backup_training_results()
    zip_path = shutil.make_archive(str(backup_dir), 'zip', root_dir=backup_dir)
    print(f"ğŸ“ ZIP path: {zip_path}")
    files.download(zip_path)

# Gá»i khi muá»‘n backup + táº£i vá» local:
backup_and_download()
```


**ChÃºc báº¡n train thÃ nh cÃ´ng! ğŸ‰**

