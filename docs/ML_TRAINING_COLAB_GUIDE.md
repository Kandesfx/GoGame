# üöÄ H∆Ø·ªöNG D·∫™N TRAINING ML TR√äN COLAB/KAGGLE

## üìë M·ª§C L·ª§C

1. [T·ªïng quan](#1-t·ªïng-quan)
2. [Setup Colab/Kaggle](#2-setup-colabkaggle)
3. [Thu th·∫≠p d·ªØ li·ªáu chuy√™n nghi·ªáp](#3-thu-th·∫≠p-d·ªØ-li·ªáu-chuy√™n-nghi·ªáp)
4. [X·ª≠ l√Ω d·ªØ li·ªáu](#4-x·ª≠-l√Ω-d·ªØ-li·ªáu)
5. [Quy tr√¨nh Training](#5-quy-tr√¨nh-training)
6. [Deployment Model](#6-deployment-model)

---

## 1. T·ªîNG QUAN

### 1.0. üöÄ QUICK START (Cho ng∆∞·ªùi ƒë√£ setup Colab)

N·∫øu b·∫°n ƒë√£:
- ‚úÖ T·∫°o notebook m·ªõi
- ‚úÖ Enable GPU
- ‚úÖ Mount Google Drive

**B∆∞·ªõc ti·∫øp theo:**

1. **T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c** (ch·∫°y Cell 1 trong template)
2. **Upload SGF Files** v√†o `GoGame_ML/raw_sgf/` (ho·∫∑c ƒë√£ c√≥ s·∫µn)
3. **Upload Code Scripts** v√†o `GoGame_ML/code/`:
   - `policy_network.py`
   - `value_network.py`
   - `generate_features_colab.py`
   - `generate_labels_colab.py`
   - `train_colab.py`
   - `parse_sgf_colab.py`
4. **Ch·∫°y theo th·ª© t·ª± c√°c cells** trong template:
   - Cell 1-2: Setup
   - Cell 3-6: Load code
   - Cell 7-8: Parse SGF ‚Üí Positions
   - Cell 9: Generate Labels
   - Cell 10: Verify Dataset
   - Cell 11-12: Training
   - Cell 13: Download Model

**Ho·∫∑c s·ª≠ d·ª•ng template:** Copy t·ª´ng cell t·ª´ `scripts/colab_notebook_template.py`

**V·ªã tr√≠ Scripts:** `scripts/` trong repository

### 1.1. T·∫°i sao d√πng Colab/Kaggle?

| Platform | GPU | Storage | Th·ªùi gian | Gi·ªõi h·∫°n |
|----------|-----|---------|-----------|----------|
| **Google Colab** | ‚úÖ Free T4 (16GB) | 15GB | 12h/session | C·∫ßn reconnect |
| **Kaggle** | ‚úÖ Free P100 (16GB) | 30GB | 9h/session | Stable h∆°n |
| **Local** | ‚ùå C·∫ßn GPU ri√™ng | Unlimited | Unlimited | T·ªën ti·ªÅn |

**Khuy·∫øn ngh·ªã**: D√πng **Kaggle** v√¨ ·ªïn ƒë·ªãnh h∆°n, ho·∫∑c **Colab Pro** ($10/th√°ng) n·∫øu c·∫ßn th·ªùi gian d√†i h∆°n.

### 1.2. Workflow t·ªïng quan

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 1: Download Professional Games           ‚îÇ
‚îÇ  ‚Ä¢ KGS Archive (70K games)                      ‚îÇ
‚îÇ  ‚Ä¢ OGS API (recent games)                       ‚îÇ
‚îÇ  ‚Ä¢ GoGoD (optional, paid)                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 2: Parse SGF ‚Üí Positions                  ‚îÇ
‚îÇ  ‚Ä¢ Extract board states                         ‚îÇ
‚îÇ  ‚Ä¢ Filter quality games                         ‚îÇ
‚îÇ  ‚Ä¢ Generate features (17 planes)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 3: Generate Labels                        ‚îÇ
‚îÇ  ‚Ä¢ Threat maps (rule-based)                      ‚îÇ
‚îÇ  ‚Ä¢ Attack maps (rule-based)                      ‚îÇ
‚îÇ  ‚Ä¢ Intent labels (pattern-based)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 4: Upload to Colab/Kaggle                 ‚îÇ
‚îÇ  ‚Ä¢ Compress dataset                              ‚îÇ
‚îÇ  ‚Ä¢ Upload to Google Drive / Kaggle Dataset       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 5: Train Model                            ‚îÇ
‚îÇ  ‚Ä¢ Load dataset                                  ‚îÇ
‚îÇ  ‚Ä¢ Train multi-task model                       ‚îÇ
‚îÇ  ‚Ä¢ Monitor v·ªõi TensorBoard                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 6: Download Model                         ‚îÇ
‚îÇ  ‚Ä¢ Save checkpoint                               ‚îÇ
‚îÇ  ‚Ä¢ Download v·ªÅ local                            ‚îÇ
‚îÇ  ‚Ä¢ Deploy v√†o backend                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.3. Y√™u c·∫ßu d·ªØ li·ªáu

| Board Size | Min Games | Min Positions | Recommended |
|------------|-----------|---------------|-------------|
| 9√ó9 | 1,000 | 80,000 | 5,000 games |
| 13√ó13 | 500 | 60,000 | 2,000 games |
| 19√ó19 | 2,000 | 400,000 | 10,000 games |

**T·ªïng c·∫ßn**: ~17,000 games chuy√™n nghi·ªáp (t·ª´ rank 5d tr·ªü l√™n)

---

## 2. SETUP COLAB/KAGGLE

### 2.1. C·∫•u tr√∫c Th∆∞ m·ª•c Google Drive (QUAN TR·ªåNG)

Tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu, h√£y t·∫°o c·∫•u tr√∫c th∆∞ m·ª•c tr√™n Google Drive nh∆∞ sau:

```
Google Drive/MyDrive/GoGame_ML/
‚îú‚îÄ‚îÄ raw_sgf/              # ‚≠ê UPLOAD SGF FILES V√ÄO ƒê√ÇY (n·∫øu c√≥)
‚îÇ   ‚îú‚îÄ‚îÄ game1.sgf
‚îÇ   ‚îú‚îÄ‚îÄ game2.sgf
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ processed/            # (T·ª± ƒë·ªông t·∫°o khi parse SGF)
‚îÇ   ‚îî‚îÄ‚îÄ positions_*.pt
‚îú‚îÄ‚îÄ datasets/             # ‚≠ê DATASET ƒê√É X·ª¨ L√ù (ƒë·ªÉ training)
‚îÇ   ‚îú‚îÄ‚îÄ positions_9x9.pt
‚îÇ   ‚îú‚îÄ‚îÄ positions_13x13.pt
‚îÇ   ‚îî‚îÄ‚îÄ positions_19x19.pt
‚îú‚îÄ‚îÄ code/                 # ‚≠ê UPLOAD CODE MODEL V√ÄO ƒê√ÇY
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multi_task_model.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ shared_backbone.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ threat_head.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ attack_head.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ intent_head.py
‚îÇ   ‚îî‚îÄ‚îÄ features.py
‚îú‚îÄ‚îÄ checkpoints/          # (T·ª± ƒë·ªông t·∫°o khi training)
‚îÇ   ‚îî‚îÄ‚îÄ best_model_epoch_X.pt
‚îú‚îÄ‚îÄ logs/                 # (T·ª± ƒë·ªông t·∫°o khi training)
‚îÇ   ‚îî‚îÄ‚îÄ TensorBoard logs
‚îî‚îÄ‚îÄ outputs/              # (T·ª± ƒë·ªông t·∫°o khi training)
    ‚îî‚îÄ‚îÄ Evaluation results
```

**L∆∞u √Ω:**
- **SGF Files**: Upload v√†o `raw_sgf/` n·∫øu b·∫°n c√≥ dataset d·∫°ng `.sgf`
- **Dataset .pt**: Upload file `.pt` ƒë√£ x·ª≠ l√Ω v√†o `datasets/` (ho·∫∑c s·∫Ω t·ª± ƒë·ªông t·∫°o t·ª´ SGF)
- **Code**: Upload c√°c file model v√†o `code/models/` v√† `code/features.py`
- C√°c th∆∞ m·ª•c `processed/`, `checkpoints/`, `logs/`, `outputs/` s·∫Ω t·ª± ƒë·ªông ƒë∆∞·ª£c t·∫°o

### 2.2. Google Colab Setup

#### B∆∞·ªõc 1: T·∫°o Notebook m·ªõi

1. V√†o https://colab.research.google.com
2. File ‚Üí New Notebook
3. ƒê·∫∑t t√™n: `GoGame_ML_Training.ipynb`

#### B∆∞·ªõc 2: Enable GPU

```python
# Cell 1: Check GPU
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
```

**Runtime ‚Üí Change runtime type ‚Üí GPU (T4)**

#### B∆∞·ªõc 3: Mount Google Drive v√† Setup C·∫•u tr√∫c Th∆∞ m·ª•c

```python
# Cell 2: Mount Drive
from google.colab import drive
drive.mount('/content/drive')

# T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c chu·∫©n
import os
from pathlib import Path

# Th∆∞ m·ª•c g·ªëc tr√™n Google Drive
WORK_DIR = Path('/content/drive/MyDrive/GoGame_ML')
WORK_DIR.mkdir(exist_ok=True)

# C·∫•u tr√∫c th∆∞ m·ª•c:
# GoGame_ML/
#   ‚îú‚îÄ‚îÄ datasets/          # Dataset ƒë√£ x·ª≠ l√Ω (upload v√†o ƒë√¢y)
#   ‚îÇ   ‚îú‚îÄ‚îÄ positions_9x9.pt
#   ‚îÇ   ‚îú‚îÄ‚îÄ positions_13x13.pt
#   ‚îÇ   ‚îî‚îÄ‚îÄ positions_19x19.pt
#   ‚îú‚îÄ‚îÄ code/              # Code model (upload v√†o ƒë√¢y)
#   ‚îÇ   ‚îú‚îÄ‚îÄ models/
#   ‚îÇ   ‚îú‚îÄ‚îÄ features.py
#   ‚îÇ   ‚îî‚îÄ‚îÄ ...
#   ‚îú‚îÄ‚îÄ checkpoints/       # Model checkpoints (t·ª± ƒë·ªông t·∫°o)
#   ‚îú‚îÄ‚îÄ logs/              # TensorBoard logs (t·ª± ƒë·ªông t·∫°o)
#   ‚îî‚îÄ‚îÄ outputs/           # K·∫øt qu·∫£ training (t·ª± ƒë·ªông t·∫°o)

# T·∫°o c√°c th∆∞ m·ª•c c·∫ßn thi·∫øt
(WORK_DIR / 'raw_sgf').mkdir(exist_ok=True)      # Cho SGF files
(WORK_DIR / 'processed').mkdir(exist_ok=True)   # Cho positions sau khi parse
(WORK_DIR / 'datasets').mkdir(exist_ok=True)    # Cho dataset ƒë√£ x·ª≠ l√Ω
(WORK_DIR / 'code').mkdir(exist_ok=True)
(WORK_DIR / 'checkpoints').mkdir(exist_ok=True)
(WORK_DIR / 'logs').mkdir(exist_ok=True)
(WORK_DIR / 'outputs').mkdir(exist_ok=True)

os.chdir(WORK_DIR)
print(f"‚úÖ Working directory: {WORK_DIR}")
print(f"üìÅ Dataset folder: {WORK_DIR / 'datasets'}")
print(f"üìÅ Code folder: {WORK_DIR / 'code'}")
```

#### B∆∞·ªõc 4: Upload Code Model (KH√îNG c·∫ßn clone git)

**C√°ch 1: Upload tr·ª±c ti·∫øp t·ª´ Colab** (Khuy·∫øn ngh·ªã cho l·∫ßn ƒë·∫ßu)

```python
# Cell 3: Upload code files
from google.colab import files
import zipfile
from pathlib import Path

print("üì§ B∆∞·ªõc 1: Upload file ZIP ch·ª©a code model")
print("   (T·∫°o ZIP t·ª´ local: zip -r gogame_ml_code.zip src/ml/models/ src/ml/features.py)")
print("   Ho·∫∑c upload t·ª´ng file ri√™ng l·∫ª")

# Option A: Upload ZIP file
uploaded = files.upload()  # Ch·ªçn file ZIP

# Extract ZIP
for filename in uploaded.keys():
    if filename.endswith('.zip'):
        with zipfile.ZipFile(filename, 'r') as zip_ref:
            zip_ref.extractall(WORK_DIR / 'code')
        print(f"‚úÖ Extracted {filename} to {WORK_DIR / 'code'}")
```

**C√°ch 2: Copy code tr·ª±c ti·∫øp v√†o Colab** (Nhanh nh·∫•t)

```python
# Cell 3: T·∫°o code files tr·ª±c ti·∫øp trong Colab
# Copy n·ªôi dung t·ª´ c√°c file trong src/ml/models/ v√† paste v√†o ƒë√¢y

# V√≠ d·ª•: T·∫°o file multi_task_model.py
code_dir = WORK_DIR / 'code'
code_dir.mkdir(exist_ok=True)

# T·∫°o __init__.py
(code_dir / '__init__.py').write_text('')

# T·∫°o th∆∞ m·ª•c models
(code_dir / 'models').mkdir(exist_ok=True)
(code_dir / 'models' / '__init__.py').write_text('')

print("üìù B√¢y gi·ªù h√£y copy n·ªôi dung t·ª´ c√°c file sau v√†o Colab:")
print("   1. src/ml/models/multi_task_model.py")
print("   2. src/ml/models/shared_backbone.py")
print("   3. src/ml/models/threat_head.py")
print("   4. src/ml/models/attack_head.py")
print("   5. src/ml/models/intent_head.py")
print("   6. src/ml/features.py")
print("\nSau ƒë√≥ ch·∫°y l·ªánh ƒë·ªÉ l∆∞u v√†o file:")
print("   %%writefile code/models/multi_task_model.py")
print("   [paste code here]")
```

**C√°ch 3: Clone t·ª´ GitHub** (N·∫øu ƒë√£ push code l√™n GitHub)

```python
# Cell 3: Clone repo (n·∫øu c√≥)
!git clone https://github.com/yourusername/GoGame.git temp_repo
!cp -r temp_repo/src/ml/models/* {WORK_DIR}/code/models/
!cp temp_repo/src/ml/features.py {WORK_DIR}/code/
!rm -rf temp_repo
print("‚úÖ Code ƒë√£ ƒë∆∞·ª£c copy v√†o code/")
```

#### B∆∞·ªõc 5: Upload Dataset (SGF ho·∫∑c .pt)

**N·∫øu b·∫°n c√≥ dataset d·∫°ng `.sgf` (Smart Game Format):**

```python
# Cell 4: Upload SGF Files
from google.colab import files
from pathlib import Path
import zipfile
import shutil

print("üì§ Upload SGF files")
print("   Option 1: Upload ZIP file ch·ª©a nhi·ªÅu .sgf files")
print("   Option 2: Upload t·ª´ng file .sgf ri√™ng l·∫ª")
print("   Option 3: N·∫øu ƒë√£ c√≥ tr√™n Google Drive, copy v√†o raw_sgf/")

# T·∫°o th∆∞ m·ª•c cho SGF files
(WORK_DIR / 'raw_sgf').mkdir(exist_ok=True)

# Option A: Upload ZIP file
uploaded = files.upload()  # Ch·ªçn file ZIP ch·ª©a .sgf files

for filename in uploaded.keys():
    if filename.endswith('.zip'):
        # Extract ZIP v√†o raw_sgf/
        with zipfile.ZipFile(filename, 'r') as zip_ref:
            zip_ref.extractall(WORK_DIR / 'raw_sgf')
        print(f"‚úÖ Extracted {filename} to raw_sgf/")
    elif filename.endswith('.sgf'):
        # Di chuy·ªÉn file .sgf v√†o raw_sgf/
        shutil.move(filename, WORK_DIR / 'raw_sgf' / filename)
        print(f"‚úÖ Moved {filename} to raw_sgf/")

# Option B: Copy t·ª´ Google Drive (n·∫øu ƒë√£ upload tr∆∞·ªõc)
# !cp -r /content/drive/MyDrive/your_sgf_folder/* {WORK_DIR}/raw_sgf/

# ƒê·∫øm s·ªë file SGF
sgf_files = list((WORK_DIR / 'raw_sgf').glob('*.sgf'))
print(f"\n‚úÖ Total SGF files: {len(sgf_files)}")
print(f"   Location: {WORK_DIR / 'raw_sgf'}")
```

**N·∫øu b·∫°n ƒë√£ c√≥ dataset d·∫°ng `.pt` (ƒë√£ x·ª≠ l√Ω s·∫µn):**

```python
# Cell 4: Upload Dataset .pt
from google.colab import files
import torch
import shutil

print("üì§ Upload dataset file (.pt)")
print("   Dataset n√™n ƒë∆∞·ª£c ƒë·∫∑t t·∫°i: datasets/positions_9x9.pt")

uploaded = files.upload()  # Ch·ªçn file .pt

for filename in uploaded.keys():
    if filename.endswith('.pt'):
        shutil.move(filename, WORK_DIR / 'datasets' / filename)
        print(f"‚úÖ Moved {filename} to datasets/")
        
        # Ki·ªÉm tra dataset
        data = torch.load(WORK_DIR / 'datasets' / filename, map_location='cpu')
        print(f"   Dataset info: {len(data.get('positions', data.get('labeled_data', [])))} samples")

# N·∫øu dataset ƒë√£ c√≥ tr√™n Google Drive
# !cp /content/drive/MyDrive/your_dataset.pt {WORK_DIR}/datasets/
```

**L∆∞u √Ω:**
- **SGF files**: C·∫ßn parse th√†nh positions tr∆∞·ªõc khi training (xem Cell 5-7)
- **.pt files**: ƒê√£ x·ª≠ l√Ω s·∫µn, c√≥ th·ªÉ training ngay (skip Cell 5-7)
- N·∫øu dataset l·ªõn (>1GB), n√™n upload l√™n Google Drive tr∆∞·ªõc, r·ªìi copy v√†o Colab

#### B∆∞·ªõc 6: Install Dependencies v√† Setup Python Path

```python
# Cell 5: Install packages
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install numpy pandas tqdm tensorboard scikit-learn
!pip install sgf  # For parsing SGF files

# Setup Python path ƒë·ªÉ import code
import sys
sys.path.insert(0, str(WORK_DIR / 'code'))
sys.path.insert(0, str(WORK_DIR / 'code' / 'models'))

print("‚úÖ Dependencies installed")
print(f"‚úÖ Python path updated: {sys.path[:3]}")
```

#### B∆∞·ªõc 7: Parse SGF ‚Üí Positions (CH·ªà C·∫¶N N·∫æU C√ì SGF FILES)

**N·∫øu b·∫°n ƒë√£ c√≥ dataset .pt, SKIP b∆∞·ªõc n√†y v√† chuy·ªÉn sang B∆∞·ªõc 8.**

```python
# Cell 6: Parse SGF Files th√†nh Positions
import sgf
import numpy as np
from pathlib import Path
from tqdm import tqdm
import torch

def parse_sgf_coord(sgf_coord, board_size):
    """Convert SGF coordinate to (x, y)"""
    if not sgf_coord or len(sgf_coord) < 2 or sgf_coord == 'tt':
        return None, None  # Pass move
    
    x = ord(sgf_coord[0]) - ord('a')
    y = ord(sgf_coord[1]) - ord('a')
    
    # Skip 'i' (no I in Go coordinates)
    if x >= 8:
        x -= 1
    if y >= 8:
        y -= 1
    
    if x < 0 or x >= board_size or y < 0 or y >= board_size:
        return None, None
    
    return x, y

def parse_sgf_file(sgf_path):
    """Parse 1 SGF file v√† extract t·∫•t c·∫£ positions"""
    try:
        with open(sgf_path, 'r', encoding='utf-8', errors='ignore') as f:
            sgf_content = f.read()
        
        # Parse SGF
        game = sgf.parse(sgf_content)
        
        # Extract metadata
        root = game.root
        board_size = int(root.properties.get('SZ', ['19'])[0])
        result = root.properties.get('RE', [''])[0]  # "B+12.5" or "W+R"
        
        # Determine winner
        if result.startswith('B'):
            winner = 'B'
        elif result.startswith('W'):
            winner = 'W'
        else:
            winner = None
        
        # Extract moves
        positions = []
        board = np.zeros((board_size, board_size), dtype=np.int8)
        current_player = 'B'  # Black starts
        
        for node in game.rest:
            # Get move
            move = None
            color = None
            
            if 'B' in node.properties:
                move = node.properties['B'][0]
                color = 'B'
            elif 'W' in node.properties:
                move = node.properties['W'][0]
                color = 'W'
            else:
                continue  # Pass or other
            
            # Parse move coordinate
            x, y = parse_sgf_coord(move, board_size)
            
            if x is not None and y is not None:
                # Save position BEFORE move
                positions.append({
                    'board_state': board.copy(),
                    'move': (x, y),
                    'current_player': current_player,
                    'move_number': len(positions),
                    'board_size': board_size,
                    'game_result': result,
                    'winner': winner
                })
                
                # Apply move (simplified - kh√¥ng x·ª≠ l√Ω captures, ko, etc.)
                board[y, x] = 1 if color == 'B' else 2
            
            current_player = 'W' if current_player == 'B' else 'B'
        
        return positions
        
    except Exception as e:
        print(f"Error parsing {sgf_path}: {e}")
        return []

# Parse t·∫•t c·∫£ SGF files
sgf_dir = WORK_DIR / 'raw_sgf'
sgf_files = list(sgf_dir.glob('*.sgf'))

print(f"üìä Parsing {len(sgf_files)} SGF files...")

all_positions = {9: [], 13: [], 19: []}

for sgf_file in tqdm(sgf_files, desc="Parsing SGF"):
    positions = parse_sgf_file(sgf_file)
    
    for pos in positions:
        board_size = pos['board_size']
        if board_size in all_positions:
            all_positions[board_size].append(pos)

# Save positions theo board size
(WORK_DIR / 'processed').mkdir(exist_ok=True)

for board_size in [9, 13, 19]:
    if all_positions[board_size]:
        output_file = WORK_DIR / 'processed' / f'positions_{board_size}x{board_size}.pt'
        torch.save({
            'positions': all_positions[board_size],
            'board_size': board_size,
            'total': len(all_positions[board_size])
        }, output_file)
        print(f"‚úÖ Saved {len(all_positions[board_size]):,} positions for {board_size}x{board_size}")

print("\n‚úÖ Parsing complete!")
```

#### B∆∞·ªõc 8: Generate Features v√† Labels (CH·ªà C·∫¶N N·∫æU C√ì SGF FILES)

```python
# Cell 7: Generate Features v√† Labels t·ª´ Positions
import torch
import numpy as np
from tqdm import tqdm

def board_to_tensor_simple(board_state, current_player, board_size):
    """Convert board state to 17-plane tensor (simplified version)"""
    features = torch.zeros((17, board_size, board_size), dtype=torch.float32)
    
    # Plane 0: Black stones
    features[0] = (board_state == 1).float()
    
    # Plane 1: White stones
    features[1] = (board_state == 2).float()
    
    # Plane 2-7: Liberty counts (simplified - c√≥ th·ªÉ c·∫£i thi·ªán sau)
    # TODO: Calculate actual liberties
    
    # Plane 8-15: History (last 4 moves, 2 planes each)
    # TODO: Track move history
    
    # Plane 16: Turn indicator
    features[16] = 1.0 if current_player == 'B' else 0.0
    
    return features

def generate_threat_map_simple(board_state, current_player):
    """Generate threat map (simplified rule-based)"""
    board_size = board_state.shape[0]
    threat_map = np.zeros((board_size, board_size), dtype=np.float32)
    
    # TODO: Implement actual threat detection
    # For now, return zeros
    return torch.from_numpy(threat_map)

def generate_attack_map_simple(board_state, current_player):
    """Generate attack map (simplified rule-based)"""
    board_size = board_state.shape[0]
    attack_map = np.zeros((board_size, board_size), dtype=np.float32)
    
    # TODO: Implement actual attack detection
    # For now, return zeros
    return torch.from_numpy(attack_map)

# Process positions v√† generate features/labels
for board_size in [9, 13, 19]:
    input_file = WORK_DIR / 'processed' / f'positions_{board_size}x{board_size}.pt'
    
    if not input_file.exists():
        continue
    
    print(f"\nüìä Processing {board_size}x{board_size}...")
    data = torch.load(input_file, map_location='cpu')
    positions = data['positions']
    
    labeled_data = []
    
    for pos in tqdm(positions, desc=f"Generating features {board_size}x{board_size}"):
        board_state = pos['board_state']
        current_player = pos['current_player']
        move = pos['move']
        
        # Generate features
        features = board_to_tensor_simple(
            torch.from_numpy(board_state),
            current_player,
            board_size
        )
        
        # Generate labels
        threat_map = generate_threat_map_simple(board_state, current_player)
        attack_map = generate_attack_map_simple(board_state, current_player)
        
        labeled_data.append({
            'features': features,
            'threat_map': threat_map,
            'attack_map': attack_map,
            'intent': {
                'type': 'unknown',  # TODO: Implement intent recognition
                'confidence': 0.5
            },
            'metadata': {
                'move_number': pos['move_number'],
                'game_result': pos['game_result'],
                'winner': pos['winner']
            }
        })
    
    # Save labeled dataset
    output_file = WORK_DIR / 'datasets' / f'positions_{board_size}x{board_size}.pt'
    torch.save({
        'labeled_data': labeled_data,
        'board_size': board_size,
        'total': len(labeled_data)
    }, output_file)
    
    print(f"‚úÖ Saved {len(labeled_data):,} labeled samples to {output_file}")

print("\n‚úÖ Feature generation complete!")
print("üìÅ Dataset ready t·∫°i: datasets/positions_*.pt")
```

#### B∆∞·ªõc 9: Verify Setup

```python
# Cell 8: Ki·ªÉm tra setup
import torch
from pathlib import Path

print("=" * 50)
print("üîç VERIFY SETUP")
print("=" * 50)

# Check GPU
print(f"\n1. GPU Check:")
print(f"   CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print(f"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

# Check code files
print(f"\n2. Code Files Check:")
code_dir = WORK_DIR / 'code'
models_dir = code_dir / 'models'
required_files = [
    'models/multi_task_model.py',
    'models/shared_backbone.py',
    'models/threat_head.py',
    'models/attack_head.py',
    'models/intent_head.py',
    'features.py'
]

all_ok = True
for file_path in required_files:
    full_path = code_dir / file_path
    exists = full_path.exists()
    status = "‚úÖ" if exists else "‚ùå"
    print(f"   {status} {file_path}")
    if not exists:
        all_ok = False

# Check datasets
print(f"\n3. Dataset Files Check:")
dataset_dir = WORK_DIR / 'datasets'
if dataset_dir.exists():
    dataset_files = list(dataset_dir.glob("*.pt"))
    if dataset_files:
        for ds_file in dataset_files:
            try:
                data = torch.load(ds_file, map_location='cpu')
                size = data.get('board_size', 'unknown')
                total = data.get('total', len(data.get('positions', data.get('labeled_data', []))))
                print(f"   ‚úÖ {ds_file.name} - Board: {size}x{size}, Samples: {total:,}")
            except Exception as e:
                print(f"   ‚ùå {ds_file.name} - Error: {e}")
    else:
        print(f"   ‚ö†Ô∏è  No dataset files found in {dataset_dir}")
        print(f"   üí° Upload dataset .pt v√†o ƒë√¢y, ho·∫∑c upload SGF files v√†o raw_sgf/ ƒë·ªÉ parse")

# Check SGF files (n·∫øu c√≥)
sgf_dir = WORK_DIR / 'raw_sgf'
if sgf_dir.exists():
    sgf_files = list(sgf_dir.glob("*.sgf"))
    if sgf_files:
        print(f"\n4. SGF Files Check:")
        print(f"   ‚úÖ Found {len(sgf_files)} SGF files in raw_sgf/")
        print(f"   üí° Run Cell 6-7 to parse SGF ‚Üí positions ‚Üí features")
else:
    print(f"\n4. SGF Files: Not found (OK if you have .pt dataset)")

# Check directories
print(f"\n5. Directories Check:")
dirs = ['raw_sgf', 'processed', 'datasets', 'checkpoints', 'logs', 'outputs']
for dir_name in dirs:
    dir_path = WORK_DIR / dir_name
    exists = dir_path.exists()
    status = "‚úÖ" if exists else "‚ùå"
    print(f"   {status} {dir_name}/")

print("\n" + "=" * 50)
if all_ok and dataset_files:
    print("‚úÖ Setup ho√†n t·∫•t! S·∫µn s√†ng ƒë·ªÉ training!")
else:
    print("‚ö†Ô∏è  C√≤n thi·∫øu m·ªôt s·ªë files. H√£y ki·ªÉm tra l·∫°i.")
print("=" * 50)
```

### 2.2. Kaggle Setup

#### B∆∞·ªõc 1: T·∫°o Notebook m·ªõi

1. V√†o https://www.kaggle.com/code
2. New Notebook
3. ƒê·∫∑t t√™n: `gogame-ml-training`

#### B∆∞·ªõc 2: Enable GPU

**Settings ‚Üí Accelerator ‚Üí GPU (P100)**

#### B∆∞·ªõc 3: Upload Dataset

1. **Data ‚Üí Add data ‚Üí New dataset**
2. Upload dataset files (s·∫Ω h∆∞·ªõng d·∫´n ·ªü ph·∫ßn sau)
3. Dataset s·∫Ω c√≥ path: `/kaggle/input/your-dataset-name/`

#### B∆∞·ªõc 4: Install Dependencies

```python
# Cell 1: Install packages
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install numpy pandas tqdm tensorboard sgf
```

#### B∆∞·ªõc 5: Setup Working Directory

```python
# Cell 2: Setup
import os
WORK_DIR = '/kaggle/working'
os.chdir(WORK_DIR)

# Copy code files (ho·∫∑c clone repo)
# !git clone https://github.com/yourusername/GoGame.git
```

---

## 3. THU TH·∫¨P D·ªÆ LI·ªÜU CHUY√äN NGHI·ªÜP

### 3.1. Ngu·ªìn d·ªØ li·ªáu

#### 3.1.1. KGS Game Archive (‚≠ê RECOMMENDED - FREE)

**URL**: https://u-go.net/gamerecords/

**Th√¥ng tin**:
- ~70,000 games chuy√™n nghi·ªáp
- Format: SGF
- Ranks: 1d - 9d professional
- **FREE v√† kh√¥ng gi·ªõi h·∫°n**

**Script download** (ch·∫°y tr√™n local tr∆∞·ªõc khi upload l√™n Colab):

```python
# scripts/download_kgs_games.py

import requests
import os
from pathlib import Path
from tqdm import tqdm
import time

KGS_BASE_URL = "https://u-go.net/gamerecords/"
OUTPUT_DIR = Path("data/raw/kgs")

def download_kgs_games(min_rank=5, max_games=10000, output_dir=OUTPUT_DIR):
    """
    Download games t·ª´ KGS Archive
    
    Args:
        min_rank: Minimum rank (dan) - ch·ªâ l·∫•y t·ª´ 5d tr·ªü l√™n
        max_games: S·ªë l∆∞·ª£ng games t·ªëi ƒëa
        output_dir: Th∆∞ m·ª•c l∆∞u
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    
    downloaded = 0
    failed = 0
    
    # KGS c√≥ nhi·ªÅu nƒÉm, m·ªói nƒÉm c√≥ nhi·ªÅu th√°ng
    years = range(2000, 2024)  # 2000-2023
    
    print(f"B·∫Øt ƒë·∫ßu download t·ª´ KGS Archive...")
    print(f"Target: {max_games} games, min rank: {min_rank}d")
    
    for year in years:
        if downloaded >= max_games:
            break
            
        for month in range(1, 13):
            if downloaded >= max_games:
                break
            
            # URL format: https://u-go.net/gamerecords/YYYY/MM/
            url = f"{KGS_BASE_URL}{year}/{month:02d}/"
            
            try:
                # Get list of SGF files
                response = requests.get(url, timeout=10)
                if response.status_code != 200:
                    continue
                
                # Parse HTML ƒë·ªÉ t√¨m links .sgf
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(response.text, 'html.parser')
                
                sgf_links = []
                for link in soup.find_all('a'):
                    href = link.get('href', '')
                    if href.endswith('.sgf'):
                        sgf_links.append(href)
                
                # Download t·ª´ng file
                for sgf_file in sgf_links:
                    if downloaded >= max_games:
                        break
                    
                    sgf_url = f"{url}{sgf_file}"
                    output_path = output_dir / f"{year}_{month:02d}_{sgf_file}"
                    
                    # Skip n·∫øu ƒë√£ c√≥
                    if output_path.exists():
                        continue
                    
                    try:
                        sgf_response = requests.get(sgf_url, timeout=10)
                        if sgf_response.status_code == 200:
                            # Check rank trong SGF metadata
                            sgf_content = sgf_response.text
                            if f"{min_rank}d" in sgf_content or f"{min_rank+1}d" in sgf_content:
                                output_path.write_text(sgf_content, encoding='utf-8')
                                downloaded += 1
                                
                                if downloaded % 100 == 0:
                                    print(f"Downloaded: {downloaded}/{max_games}")
                                
                                time.sleep(0.1)  # Rate limiting
                    except Exception as e:
                        failed += 1
                        if failed % 100 == 0:
                            print(f"Failed downloads: {failed}")
                        continue
                
            except Exception as e:
                print(f"Error processing {year}/{month}: {e}")
                continue
    
    print(f"\n‚úÖ Ho√†n th√†nh!")
    print(f"Downloaded: {downloaded} games")
    print(f"Failed: {failed}")
    print(f"Saved to: {output_dir}")

if __name__ == '__main__':
    download_kgs_games(min_rank=5, max_games=10000)
```

**C√°ch ch·∫°y**:
```bash
# Tr√™n local machine
python scripts/download_kgs_games.py --min-rank 5 --max-games 10000
```

#### 3.1.2. OGS API (FREE)

**URL**: https://online-go.com/api/v1/

**Script download**:

```python
# scripts/download_ogs_games.py

import requests
import json
from pathlib import Path
from tqdm import tqdm

OGS_API_BASE = "https://online-go.com/api/v1/"

def download_ogs_games(min_rank=5, max_games=5000, output_dir=Path("data/raw/ogs")):
    """
    Download games t·ª´ OGS API
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    
    downloaded = 0
    page = 1
    
    print(f"Downloading from OGS API...")
    
    while downloaded < max_games:
        # Get games list
        url = f"{OGS_API_BASE}games/"
        params = {
            'ordering': '-ended',
            'page': page,
            'page_size': 100,
            'ranked': True
        }
        
        try:
            response = requests.get(url, params=params, timeout=10)
            if response.status_code != 200:
                break
            
            data = response.json()
            games = data.get('results', [])
            
            if not games:
                break
            
            for game in games:
                if downloaded >= max_games:
                    break
                
                # Check rank
                black_rank = game.get('black', {}).get('ranking', 0)
                white_rank = game.get('white', {}).get('ranking', 0)
                
                if black_rank < min_rank * 100 or white_rank < min_rank * 100:
                    continue  # OGS uses numeric ranks (500 = 5d)
                
                # Download SGF
                game_id = game['id']
                sgf_url = f"{OGS_API_BASE}games/{game_id}/sgf"
                
                try:
                    sgf_response = requests.get(sgf_url, timeout=10)
                    if sgf_response.status_code == 200:
                        output_path = output_dir / f"ogs_{game_id}.sgf"
                        output_path.write_text(sgf_response.text, encoding='utf-8')
                        downloaded += 1
                        
                        if downloaded % 100 == 0:
                            print(f"Downloaded: {downloaded}/{max_games}")
                except:
                    continue
            
            page += 1
            
        except Exception as e:
            print(f"Error: {e}")
            break
    
    print(f"‚úÖ Downloaded {downloaded} games from OGS")

if __name__ == '__main__':
    download_ogs_games(min_rank=5, max_games=5000)
```

#### 3.1.3. GoGoD Database (PAID - Optional)

**URL**: https://www.gogodonline.co.uk/

**Th√¥ng tin**:
- ~100,000 historical games
- Very high quality
- Cost: ~$40 one-time
- Format: SGF

**N·∫øu mua**: Download v√† extract v√†o `data/raw/gogod/`

---

## 4. X·ª¨ L√ù D·ªÆ LI·ªÜU

### 4.1. Parse SGF ‚Üí Positions

**File**: `scripts/parse_sgf_to_positions.py`

```python
# scripts/parse_sgf_to_positions.py

import sgf
from pathlib import Path
import torch
from tqdm import tqdm
import numpy as np

def parse_sgf_file(sgf_path):
    """
    Parse 1 SGF file v√† extract t·∫•t c·∫£ positions
    
    Returns:
        List of (board_state, move, outcome) tuples
    """
    try:
        with open(sgf_path, 'r', encoding='utf-8') as f:
            sgf_content = f.read()
        
        # Parse SGF
        game = sgf.parse(sgf_content)
        
        # Extract metadata
        root = game.root
        board_size = int(root.properties.get('SZ', ['19'])[0])
        result = root.properties.get('RE', [''])[0]  # "B+12.5" or "W+R"
        
        # Determine winner
        if result.startswith('B'):
            winner = 'B'
        elif result.startswith('W'):
            winner = 'W'
        else:
            winner = None  # Unknown
        
        # Extract moves
        positions = []
        board = create_empty_board(board_size)
        current_player = 'B'  # Black starts
        
        for node in game.rest:
            # Get move
            if 'B' in node.properties:
                move = node.properties['B'][0]
                color = 'B'
            elif 'W' in node.properties:
                move = node.properties['W'][0]
                color = 'W'
            else:
                continue  # Pass or other
            
            # Parse move coordinate
            if move and move != '' and move != 'tt':  # 'tt' = pass
                x, y = parse_sgf_coord(move, board_size)
                
                # Save position BEFORE move
                positions.append({
                    'board_state': board.copy(),
                    'move': (x, y),
                    'current_player': current_player,
                    'move_number': len(positions),
                    'board_size': board_size,
                    'game_result': result,
                    'winner': winner
                })
                
                # Apply move
                board[y, x] = 1 if color == 'B' else 2
                # TODO: Apply Go rules (captures, ko, etc.)
            
            current_player = 'W' if current_player == 'B' else 'B'
        
        return positions
        
    except Exception as e:
        print(f"Error parsing {sgf_path}: {e}")
        return []

def parse_sgf_coord(sgf_coord, board_size):
    """
    Convert SGF coordinate to (x, y)
    SGF: 'aa' = (0, 0), 'sa' = (18, 0) for 19x19
    """
    if len(sgf_coord) < 2:
        return None, None
    
    x = ord(sgf_coord[0]) - ord('a')
    y = ord(sgf_coord[1]) - ord('a')
    
    # Skip 'i' (no I in Go coordinates)
    if x >= 8:
        x -= 1
    if y >= 8:
        y -= 1
    
    return x, y

def create_empty_board(size):
    """Create empty Go board"""
    return np.zeros((size, size), dtype=np.int8)

def process_all_sgf_files(sgf_dir, output_path, board_sizes=[9, 13, 19]):
    """
    Process t·∫•t c·∫£ SGF files v√† t·∫°o dataset
    
    Args:
        sgf_dir: Th∆∞ m·ª•c ch·ª©a SGF files
        output_path: Path ƒë·ªÉ l∆∞u PyTorch dataset
        board_sizes: C√°c board sizes c·∫ßn x·ª≠ l√Ω
    """
    sgf_dir = Path(sgf_dir)
    all_positions = {size: [] for size in board_sizes}
    
    sgf_files = list(sgf_dir.glob("*.sgf"))
    print(f"Found {len(sgf_files)} SGF files")
    
    for sgf_file in tqdm(sgf_files, desc="Parsing SGF"):
        positions = parse_sgf_file(sgf_file)
        
        for pos in positions:
            board_size = pos['board_size']
            if board_size in board_sizes:
                all_positions[board_size].append(pos)
    
    # Save datasets
    for board_size in board_sizes:
        if all_positions[board_size]:
            output_file = output_path / f"positions_{board_size}x{board_size}.pt"
            torch.save({
                'positions': all_positions[board_size],
                'board_size': board_size,
                'total': len(all_positions[board_size])
            }, output_file)
            print(f"‚úÖ Saved {len(all_positions[board_size])} positions for {board_size}x{board_size}")
    
    return all_positions

if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--input', type=str, required=True, help='SGF directory')
    parser.add_argument('--output', type=str, default='data/processed', help='Output directory')
    parser.add_argument('--board-sizes', type=int, nargs='+', default=[9, 13, 19])
    
    args = parser.parse_args()
    
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    process_all_sgf_files(args.input, output_dir, args.board_sizes)
```

**C√°ch ch·∫°y**:
```bash
python scripts/parse_sgf_to_positions.py \
  --input data/raw/kgs \
  --output data/processed \
  --board-sizes 9 13 19
```

### 4.2. Generate Features (17 Planes)

**File**: `src/ml/features.py` (ƒë√£ c√≥, c·∫ßn update)

```python
# src/ml/features.py (update)

import torch
import numpy as np
from typing import Tuple

def board_to_tensor(board_state: np.ndarray, current_player: str, board_size: int) -> torch.Tensor:
    """
    Convert board state to 17-plane tensor
    
    Args:
        board_state: (board_size, board_size) array, 0=empty, 1=black, 2=white
        current_player: 'B' or 'W'
        board_size: Board size
    
    Returns:
        Tensor of shape (17, board_size, board_size)
    """
    features = torch.zeros((17, board_size, board_size), dtype=torch.float32)
    
    # Plane 0: Black stones
    features[0] = (board_state == 1).float()
    
    # Plane 1: White stones
    features[1] = (board_state == 2).float()
    
    # Plane 2-7: Liberty counts (simplified - c·∫ßn implement ƒë·∫ßy ƒë·ªß)
    # TODO: Calculate actual liberties for each stone
    # For now, use simple heuristics
    
    # Plane 8-15: History (last 4 moves, 2 planes each)
    # TODO: Track move history
    
    # Plane 16: Turn indicator
    features[16] = 1.0 if current_player == 'B' else 0.0
    
    return features

def process_positions_to_features(positions_data, board_size):
    """
    Convert positions to feature tensors
    
    Args:
        positions_data: List of position dicts from parse_sgf
        board_size: Board size
    
    Returns:
        List of feature tensors
    """
    features_list = []
    
    for pos in positions_data:
        board_state = pos['board_state']
        current_player = pos['current_player']
        
        features = board_to_tensor(board_state, current_player, board_size)
        features_list.append(features)
    
    return features_list
```

### 4.3. Generate Labels

**File**: `scripts/generate_labels.py`

```python
# scripts/generate_labels.py

import torch
import numpy as np
from pathlib import Path
from tqdm import tqdm

def generate_threat_map(board_state, current_player):
    """
    Generate threat map using rule-based heuristics
    
    Returns:
        (board_size, board_size) tensor, values 0-1
    """
    board_size = board_state.shape[0]
    threat_map = np.zeros((board_size, board_size), dtype=np.float32)
    
    # TODO: Implement threat detection
    # - Groups with 1 liberty ‚Üí 1.0
    # - Groups with 2 liberties ‚Üí 0.7
    # - False eyes ‚Üí 0.6
    # - Cutting points ‚Üí 0.5
    
    return torch.from_numpy(threat_map)

def generate_attack_map(board_state, current_player):
    """
    Generate attack opportunity map
    
    Returns:
        (board_size, board_size) tensor, values 0-1
    """
    board_size = board_state.shape[0]
    attack_map = np.zeros((board_size, board_size), dtype=np.float32)
    
    # TODO: Implement attack detection
    # - Opponent in atari ‚Üí 1.0
    # - Can cut ‚Üí 0.8
    # - Invasion points ‚Üí 0.6
    # - Ladder works ‚Üí 0.7
    
    return torch.from_numpy(attack_map)

def generate_intent_label(board_state, move, prev_moves):
    """
    Generate intent label from move pattern
    
    Returns:
        intent_type: str ('territory', 'attack', 'defense', 'connection', 'cut')
        confidence: float
    """
    # TODO: Implement intent recognition
    # - Pattern matching
    # - Heuristic analysis
    
    return 'attack', 0.5  # Placeholder

def process_dataset_with_labels(input_path, output_path):
    """
    Process dataset v√† generate labels
    """
    print(f"Loading positions from {input_path}...")
    data = torch.load(input_path)
    positions = data['positions']
    board_size = data['board_size']
    
    print(f"Processing {len(positions)} positions...")
    
    labeled_data = []
    
    for pos in tqdm(positions, desc="Generating labels"):
        board_state = pos['board_state']
        current_player = pos['current_player']
        move = pos['move']
        
        # Generate features
        features = board_to_tensor(board_state, current_player, board_size)
        
        # Generate labels
        threat_map = generate_threat_map(board_state, current_player)
        attack_map = generate_attack_map(board_state, current_player)
        intent_type, intent_conf = generate_intent_label(board_state, move, [])
        
        labeled_data.append({
            'features': features,
            'threat_map': threat_map,
            'attack_map': attack_map,
            'intent': {
                'type': intent_type,
                'confidence': intent_conf
            },
            'metadata': {
                'move_number': pos['move_number'],
                'game_result': pos['game_result'],
                'winner': pos['winner']
            }
        })
    
    # Save
    torch.save({
        'labeled_data': labeled_data,
        'board_size': board_size,
        'total': len(labeled_data)
    }, output_path)
    
    print(f"‚úÖ Saved labeled dataset to {output_path}")
    print(f"Total samples: {len(labeled_data)}")

if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--input', type=str, required=True)
    parser.add_argument('--output', type=str, required=True)
    
    args = parser.parse_args()
    process_dataset_with_labels(args.input, args.output)
```

### 4.4. Prepare Dataset cho Colab/Kaggle

**Sau khi x·ª≠ l√Ω xong, compress v√† upload**:

```bash
# Compress dataset
cd data/processed
tar -czf gogame_dataset.tar.gz *.pt
# Ho·∫∑c zip
zip -r gogame_dataset.zip *.pt

# Upload l√™n Google Drive (cho Colab)
# Ho·∫∑c upload l√™n Kaggle Dataset (cho Kaggle)
```

---

## 5. QUY TR√åNH TRAINING

### 5.1. Setup tr√™n Colab

**Notebook structure**:

```python
# ============================================
# CELL 1: Setup & Install
# ============================================

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from pathlib import Path
from tqdm import tqdm
import os

# Check GPU
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}")

# Mount Drive
from google.colab import drive
drive.mount('/content/drive')

# Setup paths
WORK_DIR = Path('/content/drive/MyDrive/GoGame_ML')
WORK_DIR.mkdir(exist_ok=True)
os.chdir(WORK_DIR)

# Install packages
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install numpy pandas tqdm tensorboard sgf

print("‚úÖ Setup complete!")
```

```python
# ============================================
# CELL 2: Load Dataset
# ============================================

# Load dataset t·ª´ th∆∞ m·ª•c datasets/
dataset_path = WORK_DIR / 'datasets' / 'positions_9x9.pt'  # Thay ƒë·ªïi theo board size b·∫°n c√≥

# Load dataset
print(f"Loading dataset from {dataset_path}...")
dataset = torch.load(dataset_path, map_location='cpu')

# Dataset c√≥ th·ªÉ c√≥ format kh√°c nhau
if 'labeled_data' in dataset:
labeled_data = dataset['labeled_data']
elif 'positions' in dataset:
    # N·∫øu ch∆∞a c√≥ labels, s·∫Ω c·∫ßn generate sau
    positions = dataset['positions']
    print("‚ö†Ô∏è  Dataset ch∆∞a c√≥ labels. C·∫ßn generate labels tr∆∞·ªõc khi training.")
    labeled_data = None
else:
    raise ValueError("Dataset format kh√¥ng h·ª£p l·ªá!")

board_size = dataset['board_size']

print(f"‚úÖ Loaded dataset")
print(f"   Board size: {board_size}x{board_size}")
if labeled_data:
    print(f"   Samples: {len(labeled_data):,}")

# Split train/val/test
from sklearn.model_selection import train_test_split

train_data, temp_data = train_test_split(
    labeled_data, 
    test_size=0.2, 
    random_state=42
)
val_data, test_data = train_test_split(
    temp_data,
    test_size=0.5,
    random_state=42
)

print(f"Train: {len(train_data)}")
print(f"Val: {len(val_data)}")
print(f"Test: {len(test_data)}")
```

```python
# ============================================
# CELL 3: Create Dataset Class
# ============================================

class GoPositionDataset(Dataset):
    def __init__(self, data, augment=False):
        self.data = data
        self.augment = augment
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        
        features = sample['features']
        threat_map = sample['threat_map']
        attack_map = sample['attack_map']
        intent = sample['intent']
        
        # TODO: Add augmentation if self.augment
        
        return {
            'features': features,
            'threat_map': threat_map,
            'attack_map': attack_map,
            'intent_type': intent['type'],
            'intent_conf': intent['confidence']
        }

# Create datasets
train_dataset = GoPositionDataset(train_data, augment=True)
val_dataset = GoPositionDataset(val_data, augment=False)
test_dataset = GoPositionDataset(test_data, augment=False)

# Create dataloaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)

print("‚úÖ Datasets created")
```

```python
# ============================================
# CELL 4: Load Model Architecture
# ============================================

# Import model code t·ª´ th∆∞ m·ª•c code/
import sys
sys.path.insert(0, str(WORK_DIR / 'code'))
sys.path.insert(0, str(WORK_DIR / 'code' / 'models'))

# Import models
from models.multi_task_model import MultiTaskModel, MultiTaskConfig
# Ho·∫∑c n·∫øu ƒë√£ copy tr·ª±c ti·∫øp:
# from multi_task_model import MultiTaskModel, MultiTaskConfig

# Create model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Config model (thay ƒë·ªïi board_size theo dataset c·ªßa b·∫°n)
config = MultiTaskConfig(
    input_planes=17,
    board_size=board_size,  # S·ª≠ d·ª•ng board_size t·ª´ dataset
    base_channels=64,
    num_res_blocks=4
)

model = MultiTaskModel(config=config).to(device)

print(f"‚úÖ Model created on {device}")
print(f"   Board size: {board_size}x{board_size}")
print(f"   Parameters: {sum(p.numel() for p in model.parameters()):,}")
print(f"   Model size: {sum(p.numel() for p in model.parameters()) * 4 / 1024 / 1024:.2f} MB")
```

```python
# ============================================
# CELL 5: Training Loop
# ============================================

# Config
config = {
    'num_epochs': 50,
    'learning_rate': 1e-3,
    'weight_decay': 1e-4,
    'patience': 10,
    'checkpoint_dir': WORK_DIR / 'checkpoints',
    'log_dir': WORK_DIR / 'logs',
    'output_dir': WORK_DIR / 'outputs'
}

config['checkpoint_dir'].mkdir(exist_ok=True)
config['log_dir'].mkdir(exist_ok=True)

# Optimizer
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=config['learning_rate'],
    weight_decay=config['weight_decay']
)

# Scheduler
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max=config['num_epochs']
)

# Loss functions
threat_loss_fn = nn.MSELoss()
attack_loss_fn = nn.MSELoss()
intent_loss_fn = nn.CrossEntropyLoss()

# TensorBoard
writer = SummaryWriter(config['log_dir'])

# Training
best_val_loss = float('inf')
patience_counter = 0

for epoch in range(config['num_epochs']):
    print(f"\n=== Epoch {epoch+1}/{config['num_epochs']} ===")
    
    # Train
    model.train()
    train_loss = 0
    train_threat_loss = 0
    train_attack_loss = 0
    train_intent_loss = 0
    
    for batch in tqdm(train_loader, desc='Training'):
        features = batch['features'].to(device)
        threat_map = batch['threat_map'].to(device)
        attack_map = batch['attack_map'].to(device)
        intent_type = batch['intent_type']  # TODO: Convert to class index
        
        optimizer.zero_grad()
        
        # Forward
        outputs = model(features)
        
        # Losses
        threat_loss = threat_loss_fn(outputs['threat_map'], threat_map)
        attack_loss = attack_loss_fn(outputs['attack_map'], attack_map)
        # intent_loss = intent_loss_fn(outputs['intent_logits'], intent_type)
        
        total_loss = threat_loss + attack_loss  # + intent_loss
        
        # Backward
        total_loss.backward()
        optimizer.step()
        
        # Track
        train_loss += total_loss.item()
        train_threat_loss += threat_loss.item()
        train_attack_loss += attack_loss.item()
    
    # Average
    train_loss /= len(train_loader)
    train_threat_loss /= len(train_loader)
    train_attack_loss /= len(train_loader)
    
    # Validate
    model.eval()
    val_loss = 0
    val_threat_loss = 0
    val_attack_loss = 0
    
    with torch.no_grad():
        for batch in tqdm(val_loader, desc='Validating'):
            features = batch['features'].to(device)
            threat_map = batch['threat_map'].to(device)
            attack_map = batch['attack_map'].to(device)
            
            outputs = model(features)
            
            threat_loss = threat_loss_fn(outputs['threat_map'], threat_map)
            attack_loss = attack_loss_fn(outputs['attack_map'], attack_map)
            total_loss = threat_loss + attack_loss
            
            val_loss += total_loss.item()
            val_threat_loss += threat_loss.item()
            val_attack_loss += attack_loss.item()
    
    val_loss /= len(val_loader)
    val_threat_loss /= len(val_loader)
    val_attack_loss /= len(val_loader)
    
    # Log
    writer.add_scalar('Loss/Train', train_loss, epoch)
    writer.add_scalar('Loss/Val', val_loss, epoch)
    writer.add_scalar('Loss/Train_Threat', train_threat_loss, epoch)
    writer.add_scalar('Loss/Val_Threat', val_threat_loss, epoch)
    writer.add_scalar('Loss/Train_Attack', train_attack_loss, epoch)
    writer.add_scalar('Loss/Val_Attack', val_attack_loss, epoch)
    writer.add_scalar('LR', scheduler.get_last_lr()[0], epoch)
    
    print(f"Train Loss: {train_loss:.4f} (Threat: {train_threat_loss:.4f}, Attack: {train_attack_loss:.4f})")
    print(f"Val Loss: {val_loss:.4f} (Threat: {val_threat_loss:.4f}, Attack: {val_attack_loss:.4f})")
    
    # Save best model
    if val_loss < best_val_loss - 1e-4:
        best_val_loss = val_loss
        patience_counter = 0
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_loss': val_loss,
            'config': config
        }
        torch.save(checkpoint, config['checkpoint_dir'] / f'best_model_epoch_{epoch}.pt')
        print(f"‚úÖ Saved best model (val_loss: {val_loss:.4f})")
    else:
        patience_counter += 1
    
    # Early stopping
    if patience_counter >= config['patience']:
        print(f"Early stopping at epoch {epoch}")
        break
    
    # Scheduler step
    scheduler.step()
    
    # Periodic checkpoint
    if (epoch + 1) % 10 == 0:
        torch.save(checkpoint, config['checkpoint_dir'] / f'checkpoint_epoch_{epoch}.pt')

writer.close()
print("\n‚úÖ Training complete!")
```

```python
# ============================================
# CELL 6: Evaluate Model
# ============================================

# Load best model
best_checkpoint = torch.load(config['checkpoint_dir'] / 'best_model_epoch_X.pt')
model.load_state_dict(best_checkpoint['model_state_dict'])

# Evaluate on test set
model.eval()
test_loss = 0

with torch.no_grad():
    for batch in tqdm(test_loader, desc='Testing'):
        features = batch['features'].to(device)
        threat_map = batch['threat_map'].to(device)
        attack_map = batch['attack_map'].to(device)
        
        outputs = model(features)
        
        threat_loss = threat_loss_fn(outputs['threat_map'], threat_map)
        attack_loss = attack_loss_fn(outputs['attack_map'], attack_map)
        total_loss = threat_loss + attack_loss
        
        test_loss += total_loss.item()

test_loss /= len(test_loader)
print(f"Test Loss: {test_loss:.4f}")
```

```python
# ============================================
# CELL 7: Download Model
# ============================================

# Model s·∫Ω t·ª± ƒë·ªông l∆∞u v√†o Google Drive
# Ho·∫∑c download v·ªÅ local:
from google.colab import files

# Download checkpoint
files.download(str(config['checkpoint_dir'] / 'best_model_epoch_X.pt'))
```

---

## 6. DEPLOYMENT MODEL

### 6.1. Download Model v·ªÅ Local

```bash
# T·ª´ Google Drive
# Ho·∫∑c t·ª´ Colab: files.download()

# Save v√†o project
mkdir -p models/ml
cp best_model_epoch_X.pt models/ml/multi_task_9x9.pt
```

### 6.2. Load Model trong Backend

```python
# backend/app/services/ml_analysis_service.py

import torch
from pathlib import Path
from src.ml.models.multi_task_model import MultiTaskModel

class MLAnalysisService:
    def __init__(self, model_path: Path):
        self.device = torch.device('cpu')  # Ho·∫∑c 'cuda' n·∫øu c√≥ GPU
        self.model = MultiTaskModel(board_size=9)
        
        # Load checkpoint
        checkpoint = torch.load(model_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()
        
        print(f"‚úÖ Loaded ML model from {model_path}")
    
    def analyze(self, board_state):
        """Run inference"""
        with torch.no_grad():
            outputs = self.model(board_state)
        return outputs
```

---

## 7. CHECKLIST HO√ÄN CH·ªàNH

### Phase 1: Data Collection (Local ho·∫∑c Colab)
- [ ] Download KGS games (10,000+ games) - ho·∫∑c ƒë√£ c√≥ SGF files
- [ ] Download OGS games (5,000+ games) - ho·∫∑c ƒë√£ c√≥ SGF files
- [ ] Upload SGF files l√™n Colab v√†o `raw_sgf/` (Cell 4)
- [ ] Parse SGF ‚Üí Positions (Cell 6 tr√™n Colab)
- [ ] Filter quality games (5d+) - c√≥ th·ªÉ l√†m trong parse
- [ ] Generate features (17 planes) (Cell 7 tr√™n Colab)
- [ ] Generate labels (threat, attack, intent) (Cell 7 tr√™n Colab)
- [ ] Dataset `.pt` ƒë√£ c√≥ trong `datasets/`

### Phase 2: Upload to Colab/Kaggle
- [ ] Upload dataset to Google Drive / Kaggle Dataset
- [ ] Upload model code files
- [ ] Setup Colab/Kaggle notebook

### Phase 3: Training
- [ ] Load dataset
- [ ] Create DataLoader
- [ ] Initialize model
- [ ] Train model
- [ ] Monitor v·ªõi TensorBoard
- [ ] Save checkpoints
- [ ] Evaluate on test set

### Phase 4: Deployment
- [ ] Download model
- [ ] Test model locally
- [ ] Integrate v√†o backend
- [ ] Deploy to production

---

## 8. T√ìM T·∫ÆT V·ªä TR√ç FILES (QUAN TR·ªåNG)

### 8.1. V·ªã tr√≠ Dataset tr√™n Google Drive

**N·∫øu b·∫°n c√≥ SGF files:**

```
/content/drive/MyDrive/GoGame_ML/
‚îú‚îÄ‚îÄ raw_sgf/              ‚Üê Upload SGF files v√†o ƒë√¢y
‚îÇ   ‚îú‚îÄ‚îÄ game1.sgf
‚îÇ   ‚îî‚îÄ‚îÄ game2.sgf
‚îú‚îÄ‚îÄ processed/            ‚Üê T·ª± ƒë·ªông t·∫°o (positions sau khi parse)
‚îÇ   ‚îî‚îÄ‚îÄ positions_*.pt
‚îî‚îÄ‚îÄ datasets/             ‚Üê T·ª± ƒë·ªông t·∫°o (labeled data ƒë·ªÉ training)
    ‚îî‚îÄ‚îÄ positions_*.pt
```

**Workflow v·ªõi SGF:**
1. Upload SGF files v√†o `raw_sgf/` (Cell 4)
2. Parse SGF ‚Üí positions (Cell 6)
3. Generate features & labels (Cell 7)
4. Dataset s·∫µn s√†ng t·∫°i `datasets/` ƒë·ªÉ training

**N·∫øu b·∫°n ƒë√£ c√≥ dataset .pt:**

```
/content/drive/MyDrive/GoGame_ML/datasets/
‚îú‚îÄ‚îÄ positions_9x9.pt      ‚Üê Upload v√†o ƒë√¢y
‚îú‚îÄ‚îÄ positions_13x13.pt   ‚Üê Upload v√†o ƒë√¢y
‚îî‚îÄ‚îÄ positions_19x19.pt   ‚Üê Upload v√†o ƒë√¢y
```

**C√°ch upload .pt:**
1. Upload file `.pt` tr·ª±c ti·∫øp t·ª´ Colab: `files.upload()`
2. Ho·∫∑c copy t·ª´ Google Drive kh√°c: `!cp /path/to/dataset.pt {WORK_DIR}/datasets/`
3. Ho·∫∑c upload qua Google Drive web interface, r·ªìi copy v√†o Colab

### 8.2. V·ªã tr√≠ Code Model tr√™n Google Drive

```
/content/drive/MyDrive/GoGame_ML/code/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ multi_task_model.py
‚îÇ   ‚îú‚îÄ‚îÄ shared_backbone.py
‚îÇ   ‚îú‚îÄ‚îÄ threat_head.py
‚îÇ   ‚îú‚îÄ‚îÄ attack_head.py
‚îÇ   ‚îî‚îÄ‚îÄ intent_head.py
‚îî‚îÄ‚îÄ features.py
```

**C√°ch upload:**
1. **Option A (Khuy·∫øn ngh·ªã)**: Ch·∫°y `scripts/setup_colab_helper.py` tr√™n local ƒë·ªÉ t·∫°o ZIP, r·ªìi upload ZIP l√™n Colab
2. **Option B**: Copy code tr·ª±c ti·∫øp v√†o Colab cells v√† l∆∞u v√†o file
3. **Option C**: Clone t·ª´ GitHub (n·∫øu ƒë√£ push code)

### 8.3. Checklist Setup

**N·∫øu b·∫°n c√≥ SGF files:**

- [ ] ‚úÖ GPU enabled (T4 ho·∫∑c P100)
- [ ] ‚úÖ Google Drive mounted
- [ ] ‚úÖ Th∆∞ m·ª•c `GoGame_ML/` ƒë√£ t·∫°o v·ªõi c·∫•u tr√∫c ƒë√∫ng
- [ ] ‚úÖ SGF files ƒë√£ upload v√†o `raw_sgf/`
- [ ] ‚úÖ Code model ƒë√£ upload v√†o `code/models/`
- [ ] ‚úÖ Dependencies ƒë√£ install (bao g·ªìm `sgf` package)
- [ ] ‚úÖ Python path ƒë√£ setup ƒë√∫ng
- [ ] ‚úÖ Parse SGF ‚Üí positions (Cell 6)
- [ ] ‚úÖ Generate features & labels (Cell 7)
- [ ] ‚úÖ Dataset `.pt` ƒë√£ c√≥ trong `datasets/`
- [ ] ‚úÖ Verify setup passed (Cell 8)

**N·∫øu b·∫°n ƒë√£ c√≥ dataset .pt:**

- [ ] ‚úÖ GPU enabled (T4 ho·∫∑c P100)
- [ ] ‚úÖ Google Drive mounted
- [ ] ‚úÖ Th∆∞ m·ª•c `GoGame_ML/` ƒë√£ t·∫°o v·ªõi c·∫•u tr√∫c ƒë√∫ng
- [ ] ‚úÖ Dataset file `.pt` ƒë√£ upload v√†o `datasets/`
- [ ] ‚úÖ Code model ƒë√£ upload v√†o `code/models/`
- [ ] ‚úÖ Dependencies ƒë√£ install
- [ ] ‚úÖ Python path ƒë√£ setup ƒë√∫ng
- [ ] ‚úÖ Verify setup passed (Cell 8)

### 8.4. Script Helper

Ch·∫°y script helper tr√™n local ƒë·ªÉ chu·∫©n b·ªã files:

```bash
# Tr√™n local machine
python scripts/setup_colab_helper.py
```

Script n√†y s·∫Ω:
- ‚úÖ T·∫°o ZIP file ch·ª©a code model (`gogame_ml_code.zip`)
- ‚úÖ In h∆∞·ªõng d·∫´n setup chi ti·∫øt
- ‚úÖ T·∫°o notebook template (optional)

---

**END OF PART 1**

*Ti·∫øp t·ª•c v·ªõi ph·∫ßn chi ti·∫øt h∆°n ·ªü file ti·∫øp theo...*

